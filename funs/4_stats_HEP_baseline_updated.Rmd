---
title: "HEP_baseline_analysis"
output:
  pdf_document: default
  html_document: default
date: "2024-08-22"
editor_options: 
  chunk_output_type: inline
---

# A. Variables Overview: Bold =\> needs to be imported from other DFs

Missing variables are *marked*

1)  Self-report/Behavioral Variables:

-   Control Variables: age, sex, BMI, smoker, education_years, caffeine, heart rate knowledge, *chlorpromazine equivalent dose*, *clozapine dose*
-   Additional Control Variables: other medications, handedness, HCT strategy
-   Interoceptive Sensibility (= self-reported interoception) The Multidimensional Assessment of Interoceptive Awareness (MAIA), Body Perception Questionnaire (BPQ))
-   Interoceptive Accuracy\
    IAcc: mean percentage correctly perceived heartbeats)
-   Bodily Self-Consciousness Alterations/Depersonalization: Cambridge Depersonalization Scale (CDS)

2)  Neurophysiological Measures

-   Neural Correlates of Interception: HEP, mean amplitude 450-500 msec post- ECG R-peak
-   ECG parameters: HR, HRV (Root Mean Square of Successive Differences, RMSSD), QT interval, QTc interval, Amplitude of R wave
-   *Respiration:* ?
-   *MRI:* T1w, T2w, ASL, DTI, rsfMRI (exploratory approach focused on insula and anterior cingulate cortex (ACC) structure and function)
-   BMI (used as a control variable)

3)  *Clinical variables:*

PATIENTS - Symptoms: Severity: PANSS positive, PANSS negative, PANSS general psychopathology, PANSS total - Cognitive Impairments: BACS composite - General functioning: FROGS total

HCS - Potential psychopathology: Brief Psychiatric Rating Scale (BPRS)

NOTE: Patients' Data I need from other CSVs: BMI, education_years =\> these I added to the csv clozapine dose, chlorpromazine equivalent dose, PANSS, BACS, FROGS =\> these I will later retrieve for the correlation analyses

# B. Data Handling: General Steps

Goal: make HC and SZ data compatible, Merge Datasets, make df ready to analyse

-   Task 1: Import packages and data. Rename columns consistently.

Import libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)    # For reading CSV files
library(dplyr)    # For data manipulation
library(ggplot2)  # For plotting
library(ppcor)    # For partial correlation
library("psych")  # For partial correlation
library(VIM)      # For KNN imputation
library(caret)    # For dummy variables 
library(BayesFactor)
library(correlation)
library(see)
library(lme4)
library(emmeans)
library(apaTables)
library(psych)  
library(skimr)
library(rstatix)
library(see)
library(patchwork)
library(gtsummary)
library(tidyr)
library(gridExtra)
library(patchwork)
library(cowplot)
library(RColorBrewer)
library(lsr)
library(emmeans)
library(hms)
library(car)
library(lmtest)
library(readxl)
library(effectsize)
library("brms")



options(scipen=999)

knitr::opts_knit$set(root.dir = "/Users/denizyilmaz/Desktop/BrainTrain")

```

Import all data and tweak the columns to make them consistent as well as removing V3 and empty rows

```{r data import & skim, include=FALSE}

### 1. Import Behavioral Data
beh_data_hc <- read_xlsx('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/beh_data/BHC Healthy Controls Behavioral EEG_2025-09-04.xlsx')
# first row is descriptions so make colnames the second row
beh_data_hc$group <- "HC"
beh_data_hc <- beh_data_hc %>%
  rename(education_years = "education_years(dont count breaks)", CDS = `CDS (SUM ALL excluding 30)`)
# create BMI column for consistency btw. groups 
beh_data_hc$height <- beh_data_hc$height / 100  # Convert height from cm to m
beh_data_hc$body_mass_index <- beh_data_hc$weight / (beh_data_hc$height^2) # Calculate BMI
#make colname appropriate
colnames(beh_data_hc)[colnames(beh_data_hc) == "inclusion_criteria(1:fits,0:not)"] <- "inclusion_criteria"
# keep only those rows, where data exists
beh_data_hc <- beh_data_hc %>%
  filter(!(subject > "BHC061") & inclusion_criteria == 1 & excluded != 1) 


# import sz data
beh_data_sz <- read_xlsx('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/beh_data/Patients EEG Behavioral Data & Preprocessing Log & Data Checklist_2025-09-04.xlsx')
# first row is descriptions so make colnames the second row
colnames(beh_data_sz) <- beh_data_sz[1, ] # Assign the first row as column names
beh_data_sz <- beh_data_sz[-1, ] # Remove the first row (now used as column names)
beh_data_sz$group <- "SSD"
# make the colnames equal
beh_data_sz <- beh_data_sz %>%
  rename(sex = Gender, age = Age)
# keep only BL for SZ
beh_data_sz_bl <- beh_data_sz %>%
  rename(session = Session) %>%  # Rename the column to lowercase 'session'
  filter(session == "V1")  %>%  # Keep only rows where session equals "V1"  
  filter(!(subject > "BT067") & excluded != 1) # keep only those rows, where data exists

# Add this after importing beh_data_sz_bl
beh_data_sz_bl <- beh_data_sz_bl %>%
  mutate(group = ifelse(group == "SZ", "SSD", group))  # Convert SZ to SSD

### 2. Import clinical & cognitive data
#clinical_data <- read_excel("/Users/denizyilmaz/Desktop/BrainTrain/Behavioral_data_analysis/Data/BT_clinical_data.xlsx")
# Assign the first row as column names
#colnames(clinical_data) <- clinical_data[1, ]
# Remove the first row (now used as column names)
#clinical_data <- clinical_data[-1, ]
#make compatible
#clinical_data_V1 <- clinical_data %>%
#  rename(session = Session) %>%  # Rename the column to lowercase 'session'
#  filter(session == "V1")  %>%      # Keep only rows where session equals "V1"  
#  rename(subject = BT_ID) %>%
#  filter(!(subject > "BT053"))



# cog is not working??!!!! AAAAAAA try another excel by re downloading..
#cog_data <- read_excel("/Users/denizyilmaz/Desktop/BrainTrain/Behavioral_data_analysis/Data/BT_cog_data.xlsx")
# Assign the first row as column names
#colnames(cog_data) <- cog_data[1, ]
# Remove the first row (now used as column names)
#cog_data <- cog_data[-1, ]
#make compatible
#cog_data_V1 <- cog_data %>%
#  rename(session = Session) %>%  # Rename the column to lowercase 'session'
#  filter(session == "V1")  %>%      # Keep only rows where session equals "V1"  
#  rename(subject = BT_ID) %>%
#  filter(!(subject > "BT053"))



### 3. Import the HEP data
hep_data_hc <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/HEP_BHC/HC_hep_outputs_20250822.csv')
hep_data_hc$group <- "HC"
hep_data_sz <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/HEP_BTSCZ/hep_outputs_20250822.csv')
hep_data_sz$group <- "SSD"
# make sub names compatible
hep_data_sz$subject_id <- gsub("^BTSCZ", "BT", hep_data_sz$subject_id )
# keep only BL for SZ
hep_data_sz_bl <- hep_data_sz[hep_data_sz$session == "V1",]


### 4. Import ECG data
ecg_data_hc <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/ECG_BHC/HC_ecg_outputs_20250822.csv')
ecg_data_hc$group <- "HC"
ecg_data_sz <- read_csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/ECG_BTSCZ/SSD_ecg_outputs_20250822.csv')
ecg_data_sz$group <- "SSD"
# make subject_id compatible with the other data!
ecg_data_sz$subject_id <- gsub("^BTSCZ", "BT", ecg_data_sz$subject_id)
# keep only BL for SZ
ecg_data_sz_bl <- ecg_data_sz[ecg_data_sz$session == "V1",]



```

-   Task 2: Merge all data

```{r}

### 1. Merge Beh Data

# to verify that we did not lose columns after merging
cols_in_hc_not_sz <- setdiff(colnames(beh_data_hc), colnames(beh_data_sz_bl))  # Columns in df1 that are not in df2
cols_in_sz_not_hc <- setdiff(colnames(beh_data_sz_bl), colnames(beh_data_hc)) # Columns in df2 that are not in df1
print(cols_in_hc_not_sz)
print(cols_in_sz_not_hc)

# Concatenate the data frames vertically  
beh_data_sz_bl <- mutate_all(beh_data_sz_bl, as.character) # adjust for silly mismatches first !
beh_data_hc <- mutate_all(beh_data_hc, as.character)# adjust for silly mismatches first !
beh_data <-  bind_rows(beh_data_sz_bl, beh_data_hc)

# Concatanate clinical & beh data into one big beh df 
# beh_data <- full_join(beh_data, clinical_data_V1, by = c("subject", "session")) # or left_join bc I excluded th necessary people on the left??

### 2. Merge Physio Data

#merge
hep_data <- bind_rows(hep_data_sz_bl, hep_data_hc)
# make compatible
hep_data <- hep_data%>%   
  rename(subject = subject_id)

# merge
ecg_data_merged <- rbind(ecg_data_hc, ecg_data_sz_bl)
# make compatible
ecg_data_merged <- ecg_data_merged%>%
  rename(subject = subject_id)

# merge ecg and HEP data 
common_cols_ecg_hep <- intersect(colnames(hep_data), colnames(ecg_data_merged))
hep_ecg_data <- left_join(hep_data, ecg_data_merged, by = common_cols_ecg_hep)  #full_join, but some extra rows were in ecg bc I forgot to exclude them on the script so we will do left join...


### 3. Merge all datasets 
common_cols_beh_hep <- intersect(colnames(hep_ecg_data), colnames(beh_data))
hep_ecg_beh_data <- merge(hep_ecg_data, beh_data, by = common_cols_beh_hep)


```

-   Task 3: Verify Column/Data type (format) incl. transformations to create Dummy Variables & assign ref levels
 NOTE: If a var is numeric you should first convert it to string to be able to convert it to category.????
```{r}

# define variable types 
# those which actually look like strings
categorical_vars_string <- c("session", "group", "task", "sex", "HCT_version", "handedness")
# those which look like numbers so should be first converted to numbers to not be treated as strings e.g. 1 =/ 1.00
categorical_vars_num <- c("smoker", "had_caffeine", "HCT_counted_body", "knows_heartrate",  "blood_withdrawn", "lactate_done_before_V1", "changed_meds", "inclusion_criteria")

numeric_vars <- c("total_heart_events_nr", "total_epochs_nr", "good_epoch_count", "percentage_dropped_epochs", "Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude", "Fp2_max_amplitude", "F4_max_amplitude", "F8_max_amplitude", "Fp2_min_amplitude", "F8_min_amplitude", "Fp2_max_latency", "F4_max_latency", "F8_max_latency", "Fp2_min_latency", "F4_min_latency", "F8_min_latency", "mean_HEP_accross_channels",  "threshold_percentage_bad_epochs", "F4_min_amplitude", "Fp1_mean_HEP", "Fp2_mean_HEP", "F7_mean_HEP", "F3_mean_HEP", "Fz_mean_HEP", "F4_mean_HEP", "F8_mean_HEP", "FC5_mean_HEP", "FC1_mean_HEP", "FC2_mean_HEP", "FC6_mean_HEP", "T7_mean_HEP", "C3_mean_HEP", "Cz_mean_HEP", "C4_mean_HEP", "T8_mean_HEP", "TP9_mean_HEP", "CP5_mean_HEP", "CP1_mean_HEP", "CP2_mean_HEP", "CP6_mean_HEP", "TP10_mean_HEP", "P7_mean_HEP", "P3_mean_HEP", "Pz_mean_HEP", "P4_mean_HEP", "P8_mean_HEP", "PO9_mean_HEP", "O1_mean_HEP", "O2_mean_HEP", "PO10_mean_HEP", "heart_rate_bpm", "hrv_rmssd_ms", "R_peak_amplitude_mV", "QT_interval_ms", "QTc_interval_ms",  "new_sampling_freq", "ecg_epochs_tmin", "ecg_epochs_tmax", "age", "body_mass_index", "education_years", "BPQ_body_awareness", "BPQ_supra_diaphragmatic", "BPQ_sub_diaphragmatic", "BPQ_autonomic", "BPQ_total", "CDS", "MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting", "MAIA_total", "HCT_observed_t1", "HCT_reported_t1", "HCT_observed_t2", "HCT_reported_t2", "HCT_observed_t3", "HCT_reported_t3", "HCT_observed_t4", "HCT_reported_t4", "HCT_observed_t5", "HCT_reported_t5", "HCT_observed_t6", "HCT_reported_t6", "HCT_observed_t7", "HCT_reported_t7", "HCT_observed_t8", "HCT_reported_t8", "HCT_observed_t9", "HCT_reported_t9",  "clozapine", "days_since_last_exc", "height", "weight", "BPRS (cutoff: 20)", "PANSS_pos_P1_delusions", "PANSS_pos_P2_formal_thought_disorder", "PANSS_pos_P3_hallucination", "PANSS_pos_P4_arousal", "PANSS_pos_P5_megalomania", "PANSS_pos_P6_distrust_paranoia", "PANSS_pos_P7_hostility", "PANSS_neg_N1_flat_affect", "PANSS_neg_N2_emotional_withdrawal", "PANSS_neg_N3_lacking_affective_rapport", "PANSS_neg_N4_social_passivity_and_apathy", "PANSS_neg_N5_difficulties_in_abstract_thinking", "PANSS_neg_N6_lack_of_spontaneity_and_fluency_of_language", "PANSS_neg_N7_stereotypical_thoughts", "PANSS_pos_total", "PANSS_neg_total", "PANSS_psychopat_G1_worries_about_health", "PANSS_psychopat_G2_fear", "PANSS_psychopat_G3_guilt", "PANSS_psychopat_G4_tension", "PANSS_psychopat_G5_mannerisms_and_unnatural_posture", "PANSS_psychopat_G6_depressive_symptoms", "PANSS_psychopat_G7_motor_slowdown", "PANSS_psychopat_G8_uncooperative_behavior", "PANSS_psychopat_G9_unusal_thoughts", "PANSS_psychopat_G10_disorientation", "PANSS_psychopat_G11_lack_of_attention", "PANSS_psychopat_G12_lack_of_judgement_and_insight", "PANSS_psychopat_G13_weakness_of_will", "PANSS_psychopat_G14_lack_of_impulse_control", "PANSS_psychopat_G15_self_centredness", "PANSS_psychopat_G16_active_social_avoidance_behavior", "PANSS_psychopat", "PANSS_total", "BNSS_anhedonia", "BNSS_reaction_stressful_event", "BNSS_social_withdrawal", "BNSS_avolition", "BNSS_flattened_affect", "BNSS_alogy", "CDSS", "CGI", "GAF", "SOFAS", "FROGS_daily_life", "FROGS_activities", "FROGS_relations", "FROGS_coping", "FROGS_health_treatment", "FROGS_total", "WHOQOL_1", "WHOQOL_2", "WHOQOL_4", "WHOQOL_5", "WHOQOL_6", "WHOQOL_7", "WHOQOL_3","excluded", "excluded_ec", "excluded_eo", "excluded_hct_eeg", "excluded_hct_beh", "excluded_questionnaires")
  
string_vars <- c("subject","event_type", "event_times","epoch_time_window", "baseline_correction", "hep_time_window", "channels", "hep_max_amplitudes", "hep_max_latencies", "hep_min_amplitudes", "hep_min_latencies", "hep_mean_amplitudes", "hep_amplitudes_sd", "start_time_of_analysis", "analysis_duration", "channel_interpolated_due_too-many-bad-epochs", "epoch_strategy", "baseline", "time_window", "ecg_mean_amplitude_time_window", "ecg_sd_amplitude_time_window", "Questionnaire Notes", "EEG_date", "EEG_Time_Start", "EEG_Time_End", "eyes-closed_bad_channels", "eyes-open_bad_channels", "hct_bads","HCT_strategy", "HCT_exc_effect", "interpolated_channels", "Notes", "bad_spans", "adverse_events", "highest_edu_level", "meds", "age_mean")
  

# Specify levels for categorical variables
factor_levels <- list(
  session = c("V1"), #V3. # NA
  group = c("HC", "SSD"),
  task = c("eyes-closed", "eyes-open", "hct"), 
  sex = c("f", "m"),
  HCT_version = c("A","B"), 
  smoker = c("0","1"), 
  had_caffeine = c("0","1"), 
  HCT_counted_body = c("0","1"), # NA
  knows_heartrate = c("0","1"),#NA 
  blood_withdrawn = c("0","1"), #NA 
  lactate_done_before_V1 = c("0","1"), # NA
  changed_meds = c("0","1"), 
  excluded = c("0","1"), 
  excluded_ec = c("0","1"), 
  excluded_eo = c("0","1"), 
  excluded_hct_eeg = c("0","1"), 
  excluded_hct_beh = c("0","1"), 
  excluded_questionnaires = c("0","1"),
  handedness = c("right","left", "ambidextrous"), 
  inclusion_criteria = c("0","1") 
)


# define function to convert all cols
convert_columns <- function(df, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels) {
  # Convert string variables to character type
  for (var in string_vars) {
    if (var %in% names(df)) {
      df[[var]] <- as.character(df[[var]])
    }
  }
  
  # Convert numeric variables to numeric type
  for (var in numeric_vars) {
    if (var %in% names(df)) {
      df[[var]] <- as.numeric(df[[var]])
    }
  }
  
  # Convert variables to factors and apply levels to categorical variables
  for (var in categorical_vars_string) {
    if (var %in% names(df)) {
      df[[var]] <- as.factor(as.character(df[[var]]))  
      df[[var]] <- factor(df[[var]], levels = factor_levels[[var]])

    }
  }
    # Convert variables to factors and apply levels to categorical variables
  for (var in categorical_vars_num) {
    if (var %in% names(df)) {
      df[[var]] <- as.factor(as.double(df[[var]]))  
      df[[var]] <- factor(df[[var]], levels = factor_levels[[var]])

    }
  }
  return(df)
}



# Apply the function to your data frames
beh_data <- convert_columns(beh_data, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels)
hep_ecg_data <- convert_columns(hep_ecg_data, string_vars, numeric_vars, categorical_vars_string, categorical_vars_num, factor_levels)
hep_ecg_beh_data <- convert_columns(hep_ecg_beh_data, string_vars, numeric_vars,categorical_vars_string, categorical_vars_num, factor_levels)

# Check data types using str()
str(hep_ecg_data)
str(beh_data)
str(hep_ecg_beh_data)


"
ARCHIVE

# Apply types to columns
beh_data[categorical_vars] <- lapply(beh_data[categorical_vars], factor)
beh_data[numeric_vars] <- lapply(beh_data[numeric_vars], as.numeric)
beh_data[string_vars] <- lapply(beh_data[string_vars], as.character)

# Convert the 'group' column to a factor
beh_data$group <- as.factor(beh_data$group)

# Set 'HC' as the reference level
beh_data$group <- relevel(beh_data$group, ref = 'HC')

# make appropriate format
ecg_data_merged_bl$session <- as.character(ecg_data_merged_bl$session)
# bmi & excluded should be numeric
beh_data_sz$body_mass_index <- as.double(beh_data_sz$body_mass_index)
beh_data_sz$excluded <- as.double(beh_data_sz$excluded)
beh_data_hc$body_mass_index <- as.double(beh_data_hc$body_mass_index)
beh_data_hc$excluded <- as.double(beh_data_hc$excluded)

# handedness & sex & smoker & had_caffeine &  categorical
beh_data_hc$handedness <- as.factor(beh_data_hc$handedness)
beh_data_sz$blood_withdrawn <- as.factor(beh_data_sz$blood_withdrawn)
# Convert to POSIXlt time
beh_data_hc$EEG_Time_Start <- strptime(beh_data_hc$EEG_Time_Start, format='%H:%M:%S')
beh_data_hc$EEG_Time_End <- strptime(beh_data_hc$EEG_Time_End, format='%H:%M:%S')

beh_data_sz$EEG_Time_Start <- strptime(beh_data_sz$EEG_Time_Start, format='%H:%M:%S')
beh_data_sz$EEG_Time_End <- strptime(beh_data_sz$EEG_Time_End, format='%H:%M:%S')

# Automatically convert all possible numeric columns to numeric 
# Identify columns that can be numeric, ignoring NA values
numeric_cols <- sapply(beh_data_sz, function(x) all(is.na(x) | !is.na((as.numeric(as.character(x))))))
# numeric_cols <- sapply(beh_data_sz, function(x) all(grepl('^[0-9.-]+$', x) | is.na(x)))
# View which columns are detected as numeric-like
numeric_cols
# Convert only columns identified as numeric-like
beh_data_sz[numeric_cols] <- lapply(beh_data_sz[numeric_cols], function(x) as.numeric(as.character(x)))
# Check the structure after conversion
str(beh_data_sz)


# FUN FOR CONVERSION
convert_columns <- function(df, char_cols = NULL, factor_cols = NULL, num_cols = NULL) {
  # Convert to character
  if (!is.null(char_cols)) {
    df[char_cols] <- lapply(df[char_cols], as.character)
  }
  
  # Convert to factor
  if (!is.null(factor_cols)) {
    df[factor_cols] <- lapply(df[factor_cols], as.factor)
  }
  
  # Convert to numeric
  if (!is.null(num_cols)) {
    df[num_cols] <- lapply(df[num_cols], as.numeric)
  }
  
  return(df)
}


"

```

-   Task 4: Exclude Bad Subjects

    -   In general excluded =>
        -   if a sub is fully excluded excluded = 1

```{r}

####### FULLY EXCLUDED SUBJECTS, exclude from beh data #########

# Get the subjects where excluded is 1
excluded_subs <- beh_data$subject[beh_data$excluded == 1]
# Remove any leading or trailing whitespace and unnecessary quotes (if any)
excluded_subs_clean <- trimws(excluded_subs)
# omit na s if needed
# excluded_subs_clean <- na.omit(excluded_subs_clean)
# Print the cleaned list of excluded subjects
print(excluded_subs_clean)

# now clean the data 
beh_data <- beh_data[!(beh_data$subject %in% excluded_subs), ]
hep_ecg_data <- hep_ecg_data[!(hep_ecg_data$subject %in% excluded_subs), ]
hep_ecg_beh_data <- hep_ecg_beh_data[!(hep_ecg_beh_data$subject %in% excluded_subs), ]


```

-   Task 4: Create Indices (necessary variables that are derivatives of existing vars)

    -   IAcc: interoceptive_accuracy
    -   MAIA_total
    -   BACS Composite Cognition Scores calculation for SZ

```{r}

### Sum the specified MAIA columns to create MAIA_total
beh_data$MAIA_total <- rowSums(beh_data[, c("MAIA_noticing", "MAIA_not_distracting", 
                                    "MAIA_not_worrying", "MAIA_attention_regulation", 
                                    "MAIA_emotional_awareness", "MAIA_self_regulation", 
                                    "MAIA_body_listening", "MAIA_trusting")], na.rm = TRUE)
  

### IAcc
beh_data$interoceptive_accuracy <- numeric(length(beh_data$subject))

# Loop through each subject
for (i in 1:length(beh_data$subject)) {
  # Initialize a vector to store accuracies for each trial
  accuracies <- numeric(9)  # for the 9 trials
  
  # Loop through each trial (1 to 9)
  for (r in 1:9) {
    # Calculate the accuracy for the r-th trial
    observed_heartbeats <- beh_data[[paste0("HCT_observed_t", r)]][i]
    reported_heartbeats <- beh_data[[paste0("HCT_reported_t", r)]][i]
    
    # Calculate the accuracy for this trial
    accuracy <- 1 - abs(observed_heartbeats - reported_heartbeats) / observed_heartbeats
    
    # Store the accuracy
    accuracies[r] <- accuracy
  }
  
  # Calculate the IAcc score as the mean accuracy across all trials
  beh_data$interoceptive_accuracy[i] <- mean(accuracies, na.rm = TRUE)  # Use na.rm = TRUE to handle any missing data
}

### BACS composite
  
```

-   Task 5: Missing Data: Impute behavioral vars with at least 90% data availability. Neurophysio. data won’t be imputed, if missing, will be discarded from the given analysis.

Package to visualize missing data: *mice*, provides methods to impute the data & analyse which vars are missing & are there corr.s !!!

```{r impute missing data }

 # Calculate missing data proportions
missing_proportions <- beh_data %>%
    summarise(across(everything(), ~ mean(is.na(.)) * 100, .names = "{col}"))

# Convert missing_proportions to a tidy format
missing_proportions_long <- missing_proportions %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_proportion")

# Filter columns where missing_proportion is between 0 and 10
vars_to_impute <- missing_proportions_long %>%
  filter(missing_proportion > 0 & missing_proportion < 10) %>%
  pull(variable)

# View the result
print(vars_to_impute)
# do not impute these
no_impute_vars <- c("clozapine", "excluded_ec", "excluded_eo", "excluded_hct_eeg", "excluded_hct_beh", "excluded_questionnaires")
# Remove these variables from vars_to_impute
vars_to_impute <- setdiff(vars_to_impute, no_impute_vars)
# Print the updated list
print(vars_to_impute)

# Perform KNN imputation
if (length(vars_to_impute) == 0) {
    message("No variables with missing data to impute.")
} else {
    beh_data <- kNN(beh_data, k = 5, variable = vars_to_impute)
    colnames(beh_data) <- sub("\\.imp", "", colnames(beh_data)) # Remove the suffix added by kNN function (it adds .imp to imputed variables

}


```

-   Task 6: Create DFs per analysis by excluding subjects per ta

  -   For Questionnaires

        -   If excluded_questionnaires is 1, that row should be excluded

```{r}
# exclude people, who had unusable questionnaires
#questionnaire_data <- beh_data[beh_data$excluded_questionnaires != 1, ] # with base R

questionnaire_data <- beh_data %>% # or with dplyr
  filter(excluded_questionnaires != 1 | is.na(excluded_questionnaires))

questionnaire_data %>% 
  count(group)

# save
write.csv(questionnaire_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/questionnaire_data.csv", row.names = FALSE)

#OLD APPROACH
# check questionnaire notes, was anything strange during data collection? if so, exclude person from the following analysis.
# Filter rows where 'questionnaire_notes' is not empty
#non_empty_notes <- data[data$`Questionnaire Notes` != "" & !is.na(data$`Questionnaire Notes`), c("subject", "Questionnaire Notes")]
# View the result
# print(non_empty_notes)
```

  -   For IAcc

      -   If HCT_counted_only_body is 0 or excluded_hct_beh is 1 exclude those subs from HCT/IAcc analysis

```{r}
# not excluded due to weird stuff on data collection or bc they counted not only the body-related feelings
#iacc_data <- beh_data[beh_data$excluded_hct_beh == 0 & beh_data$HCT_counted_body == 1, ] # base R
iacc_data <- beh_data %>% # or with dplyr
  filter(excluded_hct_beh != 1 | is.na(excluded_hct_beh)) %>% 
  filter(HCT_counted_body == 1)

# From the ECG data Get only HCT we need ecg parameters of that task
hep_ecg_data_hct <- hep_ecg_data %>%
  filter(task == "hct")

# merge ECG and beh_data on common. cols to create iacc_data
common_cols <- intersect(colnames(iacc_data), colnames(hep_ecg_data_hct))

iacc_data <- left_join(iacc_data, hep_ecg_data_hct, by = common_cols) %>% #  
  mutate(group = as.factor(group))

iacc_data %>% 
  count(group)

# save
write.csv(iacc_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/iacc_data.csv", row.names = FALSE)


# WHY ARE A COUPLE PEOPLE NA? bc they were excluded subs or counted non-body also!

################# For IAcc old approach #####################

# Identify rows where HCT_counted_body is 0 in beh_data_hc and beh_data_sz
#iacc_excluded_hc <- beh_data_hc %>% 
#  filter(HCT_counted_body == 0) %>% 
#  select(subject)

#iacc_excluded_sz <- beh_data_sz %>% 
#  filter(HCT_counted_body == 0) %>% 
#  select(subject)

# Combine the results from both datasets
#iacc_excluded <- bind_rows(iacc_excluded_hc, iacc_excluded_sz) %>%
#  distinct()

# Display the excluded subject IDs
# print(iacc_excluded)
```
 
  - For EEG analysis
  
        -   If excluded_ec/excluded_eo/excluded_hct corresponding column is 1, then corresponding run has already been excluded from the EEG data.
            -   DONE during preprocessing step 1.
            
        -   Data-quality related exclusion
        
            -   BAD EPOCHS: check % excluded epochs, if \> 33%, check whether if comes from one channel, if yes interpolate that channel, if not exclude person/task.
                -   DONE during preprocessing step 3.
                
            -   BAD CHANNELS:
                -   ROI analysis: if more than 1 of ROIs interpolated, exclude run
                -   Permutation Test: exclude people with more bod channels than 33% EEG ROI


```{r}

# data containing all info for EEG analysis (HEPS, ECGs, behavioral&demographic): 
hep_ecg_beh_data


##### BAD CHANNELS #######

###  ROI DATA => if 2+ ROIs have been interpolated, remove subject from EEG analysis:

# get prep outputs
prep_output_sz <- read.csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/prep_outputs_for_analysis/SSD_prep_interp_output_sampling-250_bandpass-0.30-45.00_line-50_find_all_bads_20250821.csv')
prep_output_hc <- read.csv('/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/prep_outputs_for_analysis/HC_prep_interp_output_sampling-250_bandpass-0.30-45.00_line-50_find_all_bads_20250819.csv') ## CHECK PATHS !!

# Perform a full outer join by column names
prep_output <- dplyr::bind_rows(prep_output_sz, prep_output_hc)

# make subject col compatible
prep_output <- prep_output %>%
  rename(subject = subject_id)

# make sub names compatible
prep_output$subject <- gsub("^BTSCZ", "BT", prep_output$subject )

# Keep rows where session is not "V3" and include rows with NA in session
prep_output <- prep_output %>%
  filter(session != "V3" | is.na(session))

# Convert `analysis_duration` in both data frames to <time>
hep_ecg_beh_data <- hep_ecg_beh_data %>%
  mutate(analysis_duration = hms::as_hms(analysis_duration))

# make compatibşe data types
prep_output <- prep_output %>%
  mutate(analysis_duration = hms::as_hms(analysis_duration))

# create a big dataset including prep, hep_ecg_beh_data
#common_cols_prep_hep <- intersect(colnames(hep_ecg_beh_data), colnames(prep_output))
common_cols_prep_hep <- c("subject", "session", "task") 
hep_ecg_beh_prep_data <- left_join(hep_ecg_beh_data, prep_output, by = common_cols_prep_hep) # was full_join but all data I actly need is on the left one..


# Define the channels to check
channels_to_check <- c("F4", "F8", "Fp2")

# Create the new dataframe
roi_hep_data <- hep_ecg_beh_prep_data %>%
  filter(
    rowSums(sapply(channels_to_check, function(chan) {
      grepl(chan, interpolated_chans)
    })) +
    rowSums(sapply(channels_to_check, function(chan) {
      grepl(chan, `channel_interpolated_due_too-many-bad-epochs`)
    })) <= 1
  )

roi_hep_data %>%
  filter(task == "eyes-closed") %>%  # Keep only rows where task is "eyes-closed"
  count(group)                       # Count sample sizes per group


#### WHO DID I EXCLUDE?

# Identify subject-task combinations in hep_ecg_beh_data but not in roi_hep_data
excluded_subject_tasks <- hep_ecg_beh_data %>%
  anti_join(roi_hep_data, by = c("subject", "task")) %>%
  select(subject, task) # Select only subject and task columns

# Combine subject-task exclusions with fully excluded subjects expanded to all tasks
#final_roi_excluded_list <- excluded_subject_tasks %>%
#  bind_rows(first_excluded_with_all_tasks) %>%
#  distinct() # Remove duplicates if any
# View the final excluded list
#print(final_roi_excluded_list)

# Save to CSV if needed
write.csv(excluded_subject_tasks, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/excluded_subject_tasks.csv", row.names = FALSE)
write.csv(hep_ecg_beh_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/hep_ecg_beh_data.csv", row.names = FALSE)


```


EEG Permutation

```{r}
### PERMUTATION TEST DATA =>  exclude people with more bod channels than 33%. ... CHECK LATER IF THAT IS GOOD !!!!

# Define the threshold for percent_interpolated_electrode
threshold <- 33

# Find rows where percent_interpolated_electrode is greater than the threshold
rows_to_exclude_permutation <- prep_output$percent_interpolated_electrode > threshold

# Filter rows in `prep_output` with 2 or more channels in `channels_to_check`
prep_output_filtered_permutation <- prep_output %>%
  rowwise() %>%
  filter(percent_interpolated_electrode >= 33)

# Select subject_id and task from the filtered prep_output
excluded_subjects_tasks_permutation <- prep_output_filtered_permutation %>%
  select(subject, task)

# Filter out rows in hep_ecg_beh_data that match subject_id and task from excluded_subjects_tasks
permutation_hep_data <- hep_ecg_beh_data %>%
  anti_join(excluded_subjects_tasks_permutation, by = c("subject", "task"))

# Check the new dataset
str(permutation_hep_data)
head(permutation_hep_data)

# save 
write.csv(permutation_hep_data, "/Users/denizyilmaz/Desktop/BrainTrain/Data_analysis/permutation_hep_data.csvR", row.names = FALSE)

```


EEG Archive

```{r}

######## Archive ################
######## BAD EPOCHS ###########

# if > 33%, check whether if comes from one channel, if yes interpolate that channel (in prep), if not exclude person/task.

# Define the threshold for percent_interpolated_electrode
#threshold_bad_epoch <- 33

#Filter the dataframe to keep rows where percentage_dropped_epochs is below the threshold
#hep_ecg_beh_data_epoch_good <- hep_ecg_beh_data %>%
#  filter(percentage_dropped_epochs < threshold)


# exclude people/task to be excluded 
# Filter out rows based on the specified conditions
hep_ecg_beh_prep_data_exclusion <- hep_ecg_beh_prep_data %>%
  filter(
    # Keep rows where 'excluded' is not 1
    excluded != 1,
    
    # Keep rows where 'excluded_ec' is not 1 when 'task' is 'eyes-closed'
    !(excluded_ec == 1 & task == "eyes-closed"),
    
    # Keep rows where 'excluded_eo' is not 1 when 'task' is 'eyes-open'
    !(excluded_eo == 1 & task == "eyes-open"),
    
    # Keep rows where 'excluded_hct_eeg' is not 1 when 'task' is 'hct'
    !(excluded_hct_eeg == 1 & task == "hct")
  )


## It would be better to sequentially extend this list to see in whch step who get excluded or better REMOVE THEM ALREADY IN EEG STEPS !!
# Filter rows in `prep_output` with 2 or more channels in `channels_to_check`
prep_output_filtered <- prep_output %>%
  rowwise() %>%
  mutate(channels_count = sum(intersect(channels_to_check, unlist(strsplit(as.character(interpolated_chans), ", "))) %in% channels_to_check)) %>%
  ungroup() %>%
  filter(channels_count >= 2)

# Select subject_id and task from the filtered prep_output
excluded_subjects_tasks <- prep_output_filtered %>%
  select(subject, task)

#roi_hep_data <- hep_ecg_beh_data %>%
#  anti_join(excluded_subjects_tasks, by = c("subject", "task"))

######### determine all excluded ppl

# Apply all exclusion criteria
hep_ecg_beh_prep_data_exclusion <- hep_ecg_beh_prep_data %>%
  filter(
    percentage_dropped_epochs < threshold,         # Criterion 1: Bad epochs threshold
    excluded != 1,                                 # Criterion 2: General exclusion
    !(excluded_ec == 1 & task == "eyes-closed"),   # Criterion 3: Eyes-closed exclusion
    !(excluded_eo == 1 & task == "eyes-open"),     # Criterion 4: Eyes-open exclusion
    !(excluded_hct_eeg == 1 & task == "hct")       # Criterion 5: HCT exclusion
  )

# Identify excluded subjects and tasks
excluded_subjects_tasks <- hep_ecg_beh_prep_data %>%
  anti_join(hep_ecg_beh_prep_data_exclusion, by = c("subject", "task")) %>%
  select(subject, task)

# Save excluded subjects and tasks to a CSV file
write.csv(excluded_subjects_tasks, "excluded_subjects_tasks.csv", row.names = FALSE)


######## ICA OUTPUTS ... in case needed ....change.... #########
ica_output_sz <- read.csv("/Users/denizyilmaz/Desktop/BrainTrain/BrainTrain_EEG_data/Preprocessed_ICA_applied_on_raw/ica_output_with_find-bads-ecg_raw_preprocessed_2025-02-13.csv")
ica_output_hc <- read.csv('/Users/denizyilmaz/Desktop/BrainTrain/Healthy Controls_BHC/BrainTrain_EEG_data_HC/HC_Preprocessed_ICA_applied_on_raw/HC_ica_output_with_find-bads-ecg_raw_preprocessed_2025-02-13.csv') ## CHECK PATHS !!

```

-   Correlations

```{r}

# Corr of questionnaires & iacc 
corr_questionnaire_iacc_data <- iacc_data %>% 
  filter(excluded_questionnaires != 1 | is.na(excluded_questionnaires))

# Corr of clinical & questionnaires
corr_questionnaire_clinical_data <- questionnaire_data %>%
  filter(group == "SSD")

# Corr of clinical & iacc
corr_iacc_clinical_data <- iacc_data %>%
  filter(group == "SSD")

# Corr of questionnaires & HEP ROI
corr_questionnaire_hep_roi_data <- roi_hep_data %>%
  left_join(questionnaire_data, by = c("subject", "session"))

# Corr of IAcc & HEP ROI
corr_iacc_hep_roi_data <- roi_hep_data %>%
  left_join(iacc_data, by = c("subject", "session"))

# Corr of clinical & HEP ROI 
corr_clinical_hep_roi_data <- roi_hep_data %>%
  left_join(beh_data, by = c("subject", "session"))


      


```

-   Task 8: Inspect Variable Distributions to see if everything looks normal (any implausible data? outliers?) OR do before each analysis??!!

    -   Inspect Descriptives/Distributions

```{r}

#skim
skim(beh_data)

#descriptives
descriptive_table <- beh_data %>% 
  get_summary_stats()

# Summarize without NA values for mean, median, etc.
descriptive_table_no_na <- beh_data %>%
  summarise(across(where(is.numeric), list(mean = ~mean(. , na.rm = TRUE), 
                                           sd = ~sd(. , na.rm = TRUE),
                                     median = ~median(. , na.rm = TRUE))))


# skim by group
beh_data %>%
  dplyr::group_by(group) %>%
  skim()

group_counts <- table(beh_data$group)
print(group_counts)

# get group counts
beh_data %>%
  count(group)
beh_data %>%
  count(sex)

# Check sex distribution by group
beh_data %>%
  group_by(group) %>%
  count(sex) %>%
  spread(sex, n, fill = 0)

beh_data %>%
  group_by(group) %>%
  count(sex) %>%
  mutate(proportion = n / sum(n)) %>%
  spread(sex, proportion, fill = 0)


# Create density plot for each group
ggplot(data = beh_data, aes(x = education_years, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of education_years per Group",
       x = "education_years",
       y = "Density",
       color = "Group") +
  theme_minimal()

# Create density plot for each group
ggplot(data = beh_data, aes(x = body_mass_index, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of body_mass_index per Group",
       x = "body_mass_index",
       y = "Density",
       color = "Group") +
  theme_minimal()
  
# Create density plot for each group
ggplot(data = beh_data, aes(x = age, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of age per Group",
       x = "age",
       y = "Density",
       color = "Group") +
  theme_minimal()

# Create density plot for each group
ggplot(data = beh_data, aes(x = BPQ_total, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of BPQ_total per Group",
       x = "BPQ_total",
       y = "Density",
       color = "Group") +
  theme_minimal()

# Create density plot for each group
ggplot(data = beh_data, aes(x = interoceptive_accuracy, color = group, group = group)) +
  geom_density() +
  labs(title = "Density Plot of interoceptive_accuracy per Group",
       x = "interoceptive_accuracy",
       y = "Density",
       color = "Group") +
  theme_minimal()

```

```{r fun to inspect => use this}

# Investigate ungrouped distribution of all vars in data
investigate_distribution_simple <- function(data, vars) {
  # Ensure vars exist in the dataset
  vars <- vars[vars %in% colnames(data)]
  
  if (length(vars) == 0) {
    stop("None of the specified variables are present in the dataset.")
  }
  
  # Generate density plots
  for (var in vars) {
    plot <- ggplot(data, aes(x = .data[[var]])) +
      geom_density(fill = "blue", alpha = 0.4) +
      labs(
        title = paste("Density Plot of", var),
        x = var,
        y = "Density"
      ) +
      theme_minimal()
    print(plot)
  }
}


beh_data %>%
  dplyr::group_by(group) %>%
  skim()


```

```{r fun to inspect by group => use this too}
# Investigate grouped distribution of all vars in data
investigate_distribution_grouped <- function(data, vars, group_var) {
  # Ensure vars and group_var exist in the dataset
  vars <- vars[vars %in% colnames(data)]
  if (!group_var %in% colnames(data)) {
    stop(paste("The specified group variable", group_var, "is not present in the dataset."))
  }
  if (length(vars) == 0) {
    stop("None of the specified variables are present in the dataset.")
  }
  
  # Generate density plots grouped by group_var
  for (var in vars) {
    plot <- ggplot(data, aes(x = .data[[var]], color = .data[[group_var]], group = .data[[group_var]])) +
      geom_density() +
      labs(
        title = paste("Density Plot of", var, "by", group_var),
        x = var,
        y = "Density",
        color = group_var
      ) +
      theme_minimal()
    print(plot)
    
    # Skim grouped summary
    summary_tbl <- data %>%
      group_by(.data[[group_var]]) %>%
      skimr::skim(.data[[var]])
    
    print(summary_tbl)
  }
}
```

```{r summarize fun}
summarize_data <- function(data, numeric_vars, group_vars) {
  
  # Filter variables to include only those that exist in the dataset
  numeric_vars <- numeric_vars[numeric_vars %in% colnames(data)]
  group_vars <- group_vars[group_vars %in% colnames(data)]

  # 1. Skim
  print(skim(data))
  
  # 2. Descriptive statistics
  descriptive_table <- data %>% 
    summarise(across(all_of(numeric_vars), list(mean = ~mean(. , na.rm = TRUE), 
                                                sd = ~sd(. , na.rm = TRUE),
                                                median = ~median(. , na.rm = TRUE))))
  print(descriptive_table)
  
  # 3. Grouped descriptives
  data %>%
    group_by(across(all_of(group_vars))) %>%
    skim() %>%
    print()
  
  # 4. Group counts
  group_counts <- data %>%
    count(across(all_of(group_vars))) 
  print(group_counts)
  
  # 5. Sex distribution by group
  if ("sex" %in% colnames(data)) {
    data %>%
      group_by(across(all_of(group_vars))) %>%
      count(sex) %>%
      mutate(proportion = n / sum(n)) %>%
      spread(sex, proportion, fill = 0) %>%
      print()
  }
  
  # 6. Density plots for all numeric variables
  for (var in numeric_vars) {
    ggplot(data, aes_string(x = var, color = group_vars, group = group_vars)) +
      geom_density() +
      labs(title = paste("Density Plot of", var, "per Group"),
           x = var,
           y = "Density",
           color = "Group") +
      theme_minimal() +
      print()
  }
}

```

```{r}

# Define numeric variables 
numeric_vars <- c(
  "BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", 
  "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic", 
  "MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", 
  "MAIA_attention_regulation", "MAIA_emotional_awareness", 
  "MAIA_self_regulation", "MAIA_body_listening", "MAIA_trusting", 
  "MAIA_total", "CDS", "education_years", "body_mass_index", 
  "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", 
  "QTc_interval_ms", "R_peak_amplitude_mV", "interoceptive_accuracy",
  "Fp2_mean_amplitude","F4_mean_amplitude", "F8_mean_amplitude"
)
  # Define group variables
group_vars <- c("group", "task", "sex", "session", "smoker", "had_caffeine")

summarize_data(questionnaire_data, numeric_vars, group_vars)
summarize_data(iacc_data, numeric_vars, group_vars)
summarize_data(roi_hep_data, numeric_vars, group_vars)

```

-   Task 9: Check assumptions of the test you want to conduct!

CHECK ASSUMPTIONS OF EACH TEST

Assumptions for lm() (Linear Regression) 1. Linearity The relationship between predictors and the outcome variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The residuals have constant variance across all levels of the predictors. 4. Normality of Residuals The residuals are normally distributed. 5. No Multicollinearity Predictors are not highly correlated with each other. 6. No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

Assumptions for ANCOVA 1. Linearity The relationship between the covariate(s) and the dependent variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The variance of the dependent variable is the same across all groups. 4. Normality of Residuals The residuals are normally distributed within each group. 5. Homogeneity of Regression Slopes The relationship between the covariate(s) and the dependent variable is the same across groups. (This is specific to ANCOVA.) 6. No Multicollinearity (If Multiple Covariates) Covariates are not strongly correlated with each other. 7. No Outliers or High Leverage Points There are no extreme outliers or influential points that could distort the analysis.

Count GROUPS

```{r}
questionnaire_data |> group_by(group) |> pull(subject) |> unique()
```

# C. Hypothesis Testing

## 1) We hypothesize that individuals with SSD show altered interoception on three levels compared to healthy controls (HC):

### 1.1) Interoceptive sensibility differences measured by interoception questionnaires Multidimensional Assessment of Interoceptive Awareness (MAIA) and Body Perception Questionnaire (BPQ). The Cambridge Depersonalization Scale (CDS) will capture further differences in bodily self-consciousness. (Non-directional hypothesis)

We will build a regression model for each hypothesis, where the variables of interest (Interoceptive Sensibility and Interoceptive Accuracy data) will be the dependent variable and the diagnostic group (HC vs. SSD) will be the predictor, along with the covariates of age, gender, BMI, and years of education.

Define Questionnaires

```{r define questionnaire_vars and questionnaire_covars}
questionnaire_vars <- c("BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic","MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "MAIA_total", "CDS")

questionnaire_vars_totals <- c("BPQ_total", "MAIA_total", "CDS")

questionnaire_vars_no_totals <- c("BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic","MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation",         "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "CDS")

BPQ_vars <- c("BPQ_total", "BPQ_body_awareness", "BPQ_autonomic", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic")
BPQ_vars_no_total <- c("BPQ_body_awareness", "BPQ_sub_diaphragmatic", "BPQ_supra_diaphragmatic")


MAIA_vars <- c("MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting", "MAIA_total")
MAIA_vars_no_total <- c("MAIA_noticing", "MAIA_not_distracting", "MAIA_not_worrying", "MAIA_attention_regulation", "MAIA_emotional_awareness", "MAIA_self_regulation","MAIA_body_listening", "MAIA_trusting")


questionnaire_covars_numeric = c("age", "education_years", "body_mass_index")

questionnaire_covars_categorical = c("group", "sex")


```

INSPECT

```{r}
investigate_distribution_simple(data = questionnaire_data, vars = questionnaire_vars)
investigate_distribution_grouped(data = questionnaire_data, vars = questionnaire_vars, group_var = "group")
investigate_distribution_grouped(data = questionnaire_data, vars = questionnaire_covars_numeric, group_var = "group")

# also check freqs of categorical vars
questionnaire_data %>%
  group_by(group) %>%
  count(sex) %>%
  mutate(proportion = n / sum(n)) %>%
  spread(sex, proportion, fill = 0)


```

#### Check for Assumptions for lm() (Linear Regression)

1.  Linearity The relationship between predictors and the outcome variable is linear.
2.  Independence of Observations Observations are independent of each other.
3.  Homoscedasticity The residuals have constant variance across all levels of the predictors.
4.  Normality of Residuals The residuals are normally distributed.
5.  No Multicollinearity Predictors are not highly correlated with each other.
6.  No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

#### TESTS

TEST one subscale

```{r lm BPQ_total }

# compute LMM and save statistics of main effect as data frame
lm_result <- lm(BPQ_total ~ group +age + sex + education_years + body_mass_index, 
               data=questionnaire_data,
               na.action=na.omit)


# get the model fits:  AIC (Akaike Information Criterion), or BIC (Bayesian Information Criterion)
#AIC(lm_result)
#BIC(lm_result)

# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)

# anova() Computes analysis of variance (or deviance) tables for one or more fitted model objects.
anova_table <- anova(lm_result)

# Add predictor and outcome columns
anova_table$predictor <- rownames(anova_table)
rownames(anova_table) <- NULL # now we have a col for predictors, dont need the rownames
anova_table$outcome <- rep("brainage", times = nrow(anova_table))
# df_den
n <- nobs(lm_result)
k <- length(levels(questionnaire_data$group))
df_den = n - k
anova_table$df_den <- df_den

# Rename the columns appropriately
colnames(anova_table) = c("df_num","sum_sq","mean_sq","f","p","predictor","outcome", "df_den")

# Reorder the columns
anova_table = anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

# Format p-values to avoid scientific notation
anova_table$p <- format(anova_table$p, scientific = FALSE, digits = 3)
# Format p-values to exactly three digits after the decimal point
# anova_table$p <- sprintf("%.3f", as.numeric(anova_table$p))
```

Test assumptions of lm
```{r}

# Linearity
crPlots(lm_result) # This helps identify if the relationship between predictors and the outcome is linear.

# Homoscedasticity
bptest(lm_result) # Breusch-Pagan Test: testing for homoscedasticity: p < 0.05: Evidence of heteroscedasticity, consider transforming the dependent variable. same thing:ncvTest(lm_result)

# Normality of Residuals
shapiro.test(residuals(lm_result)) # p < 0.05 => Residuals deviate from normality

# Collinearity
vif(lm_result) # VIF > 5: Considered problematic multicollinearity.; VIF > 10: Very severe multicollinearity, drop or combine predictors.

# Model specification test
resettest(lm_result) # p < 0.05: Model might be misspecified (e.g., nonlinear relationships, omitted predictors).

# Outlier Test
outlierTest(lm_result) # it should say: No Studentized residuals with Bonferroni p < 0.05



```

assumptions fun
```{r}
check_lm_assumptions <- function(model, data) {
  library(car)  # For VIF and crPlots
  library(lmtest)  # For bptest and resettest
  library(nortest)  # For Shapiro-Wilk test

  results <- list()

  # Linearity
  message("Checking Linearity...")
  results$linearity <- tryCatch({
    crPlots(model)
    "Linearity plot displayed."
  }, error = function(e) {
    "Could not generate linearity plot. Check your predictors."
  })

  # Homoscedasticity
  message("Checking Homoscedasticity...")
  results$homoscedasticity <- tryCatch({
    bp <- bptest(model)
    if (bp$p.value > 0.05) {
      "Pass: Residuals are homoscedastic (Breusch-Pagan test p > 0.05)."
    } else {
      "Fail: Residuals show heteroscedasticity (Breusch-Pagan test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Breusch-Pagan test."
  })

  # Normality of Residuals
  message("Checking Normality of Residuals...")
  results$normality <- tryCatch({
    shapiro <- shapiro.test(residuals(model))
    if (shapiro$p.value > 0.05) {
      "Pass: Residuals are approximately normal (Shapiro-Wilk test p > 0.05)."
    } else {
      "Fail: Residuals deviate from normality (Shapiro-Wilk test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Shapiro-Wilk test."
  })

  # Multicollinearity
  message("Checking Multicollinearity...")
  results$multicollinearity <- tryCatch({
    vif_values <- vif(model)
    problematic <- names(vif_values[vif_values > 5])
    if (length(problematic) == 0) {
      "Pass: No problematic multicollinearity (all VIFs < 5)."
    } else {
      paste("Fail: Problematic multicollinearity detected for", paste(problematic, collapse = ", "), ".")
    }
  }, error = function(e) {
    "Could not compute VIF."
  })

  # Model Specification
  message("Checking Model Specification...")
  results$model_specification <- tryCatch({
    reset <- resettest(model)
    if (reset$p.value > 0.05) {
      "Pass: Model appears well-specified (RESET test p > 0.05)."
    } else {
      "Fail: Model may be misspecified (RESET test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform RESET test."
  })

  # Outliers
  message("Checking Outliers...")
  results$outliers <- tryCatch({
    outlier <- outlierTest(model)
    if (is.null(outlier)) {
      "Pass: No significant outliers detected (Bonferroni p > 0.05)."
    } else {
      "Fail: Significant outliers detected. Inspect Studentized residuals."
    }
  }, error = function(e) {
    "Could not perform outlier test."
  })

  # Return all results
  return(results)
}

```

assumptions fun with plots
```{r}
check_lm_assumptions <- function(model, data) {
  library(car)       # For VIF and crPlots
  library(lmtest)    # For bptest, resettest, and raintest
  library(nortest)   # For Shapiro-Wilk test

  results <- list()

  # Linearity
  message("Checking Linearity...")
  results$linearity <- tryCatch({
    crPlots(model)  # Component + Residual Plots
    raintest_result <- raintest(model)  # Rainbow test
    if (raintest_result$p.value > 0.05) {
      "Pass: No evidence of nonlinearity (Rainbow test p > 0.05)."
    } else {
      "Fail: Evidence of nonlinearity (Rainbow test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not generate linearity test or plots."
  })

  # Homoscedasticity
  message("Checking Homoscedasticity...")
  results$homoscedasticity <- tryCatch({
    bp <- bptest(model)
    if (bp$p.value > 0.05) {
      "Pass: Residuals are homoscedastic (Breusch-Pagan test p > 0.05)."
    } else {
      "Fail: Residuals show heteroscedasticity (Breusch-Pagan test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Breusch-Pagan test."
  })

  # Normality of Residuals
  message("Checking Normality of Residuals...")
  results$normality <- tryCatch({
    shapiro <- shapiro.test(residuals(model))
    if (shapiro$p.value > 0.05) {
      "Pass: Residuals are approximately normal (Shapiro-Wilk test p > 0.05)."
    } else {
      "Fail: Residuals deviate from normality (Shapiro-Wilk test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform Shapiro-Wilk test."
  })

  # Multicollinearity
  message("Checking Multicollinearity...")
  results$multicollinearity <- tryCatch({
    vif_values <- vif(model)
    problematic <- names(vif_values[vif_values > 5])
    if (length(problematic) == 0) {
      "Pass: No problematic multicollinearity (all VIFs < 5)."
    } else {
      paste("Fail: Problematic multicollinearity detected for", paste(problematic, collapse = ", "), ".")
    }
  }, error = function(e) {
    "Could not compute VIF."
  })

  # Model Specification
  message("Checking Model Specification...")
  results$model_specification <- tryCatch({
    reset <- resettest(model)
    if (reset$p.value > 0.05) {
      "Pass: Model appears well-specified (RESET test p > 0.05)."
    } else {
      "Fail: Model may be misspecified (RESET test p <= 0.05)."
    }
  }, error = function(e) {
    "Could not perform RESET test."
  })

  # Outliers
  message("Checking Outliers...")
  results$outliers <- tryCatch({
    outlier <- outlierTest(model)
    if (is.null(outlier)) {
      "Pass: No significant outliers detected (Bonferroni p > 0.05)."
    } else {
      "Fail: Significant outliers detected. Inspect Studentized residuals."
    }
  }, error = function(e) {
    "Could not perform outlier test."
  })

  # Diagnostic Plots
  message("Generating Diagnostic Plots...")
  tryCatch({
    par(mfrow = c(2, 2))
    plot(model, which = 1:4)  # Generates Residuals vs Fitted, QQ, Scale-Location, and Residuals vs Leverage plots
    par(mfrow = c(1, 1))
  }, error = function(e) {
    message("Could not generate diagnostic plots.")
  })

  
    message("Generating Linearity Plots...")
    tryCatch({
    crPlots(model)
    "Linearity plot displayed."
  }, error = function(e) {
    "Could not generate linearity plot. Check your predictors."
  })
  # Return all results
  return(results)
}

```

try whether the fun works

```{r}
check_lm_assumptions(lm_model,questionnaire_data)
```

TEST ALL MAIN HYPOTHESES LM (Totals): correct p for 3 questionnaires!
```{r}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/questionnaires"


# Initialize a list to store the results
combined_results <- list()

for (var in questionnaire_vars_totals) { # or questionnaire_vars
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  # Get ANOVA table
  #anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  #anova_table$predictor <- rownames(anova_table)
  #rownames(anova_table) <- NULL
  #anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  #n <- nobs(lm_result)
  #k <- length(levels(questionnaire_data$group))
  #df_den <- n - k
  #anova_table$df_den <- df_den
  
  # Rename columns
  #colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  #anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
  # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate_b", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (std-beta)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate_beta = Std_Coefficient,
    std_beta_95CI_low = CI_low,
    std_beta_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = var) %>%
  select(predictor, outcome, std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)

  
  # Merge the ANOVA and coefficient tables on the predictor variable
  #combined_table <- full_join(anova_table, coef_df, effect_size_df, by = c("outcome", "predictor"))
  combined_table <- full_join(std_beta_df, coef_df, by = c("outcome", "predictor"))
  #full_join(effect_size_df, by = c("outcome", "predictor"), all = TRUE)
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
  
  # get emmeans
  emm <- emmeans(lm_result, ~ group )
  pairwise_results <- pairs(emm)
  summary(emm)
  plot(emm)

  # Convert emmeans results to a data frame
  emm_df <- as.data.frame(emm)
  
}


# Combine all results into one dataframe
lm_results_df_questionnaires_total <- do.call(rbind, combined_results)
#write.csv(anova_results_df_questionnaires_total, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Sensibility/MBB_results/lm_results_questionnaires_group_p-adjusted_2025-02-21.csv", row.names = FALSE)
write.csv(lm_results_df_questionnaires_total, file = file.path(plot_path, "lm_results_df_questionnaires_total_2025-05-06.csv"),row.names = FALSE)


# make a df only for group comparison, get only results of interest Filter the results where the predictor "groupSZ" and "group"
lm_results_df_questionnaires_total_groupSZ <- lm_results_df_questionnaires_total[
  lm_results_df_questionnaires_total$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
lm_results_df_questionnaires_total_groupSZ$p_adjusted <- p.adjust(lm_results_df_questionnaires_total_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_questionnaires_total_groupSZ)

# SAVE !!!
write.csv(lm_results_df_questionnaires_total_groupSZ, file = file.path(plot_path, "lm_results_df_questionnaires_total_p-adjusted_2025-05-06.csv"),row.names = FALSE)


```

TEST All Main Hypotheses with assumption checks: correct p for 3 questionnaires! THIS is GOOOOD!!!! 
```{r}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires/"

# Initialize a list to store the results
combined_results <- list()
combined_emms <- list()

for (var in questionnaire_vars_totals) { # or questionnaire_vars
  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  # Identify rows used in the linear model
  used_rows <- !is.na(questionnaire_data$group) & 
                complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])
  
  # Create a new dataframe with residuals and corresponding groups
  residuals_data <- data.frame(
    group = questionnaire_data$group[used_rows],
    residuals = residuals(lm_result)
  )
  
  # Step 2: Create a boxplot of residuals by group => To understand how the unexplained variance looks, NOT for the paper! 
  p <- ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Boxplot without separate outlier points
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +  # Show individual data points
    labs(title = paste("Distribution of Residuals by Group for", var), 
         x = "Group", 
         y = "Residuals") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set3")  # Optional: better color scheme
  
  print(p)
  
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Residuals_by_Group_", var, ".png"),
         plot = p,
         width = 8, height = 6, dpi = 300)

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
  # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Get 95% CI
  ci_df <- as.data.frame(confint(lm_result))
  ci_df$predictor <- rownames(ci_df)
  rownames(ci_df) <- NULL
  colnames(ci_df) <- c("CI_lower", "CI_upper", "predictor")
  
  # Merge CI with coefficients
  coef_df <- merge(coef_df, ci_df, by = "predictor")
  
   # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(lm_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  #combined_table <- full_join(anova_table, coef_df, effect_size_df, by = c("outcome", "predictor"))
  combined_table <- full_join(anova_table, coef_df, by = c("outcome", "predictor")) %>%
  full_join(effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
  
  # get emmeans
  emm <- emmeans(lm_result, ~ group )
  pairwise_results <- pairs(emm)
  summary(emm)
  plot(emm)

  # Convert emmeans results to a data frame
  emm_df <- as.data.frame(emm)
  
  # Add predictor and outcome columns to the coefficients data frame
  emm_df$predictor <- rownames(emm_df)
  rownames(emm_df) <- NULL
  emm_df$outcome <- rep(var, times = nrow(emm_df))
  
  # Store the combined results in the list
  combined_emms[[var]] <- emm_df
  
}

# Combine all results into one dataframe
anova_results_df_questionnaires_total <- do.call(rbind, combined_results)
anova_results_emms_questionnaires_total <- do.call(rbind, combined_emms)


# Save the data frame as a CSV file to the directory
write.csv(anova_results_df_questionnaires_total, file = file.path(plot_path, "lm_results_questionnaires_2025-09-05.csv"), row.names = FALSE)
write.csv(anova_results_emms_questionnaires_total, file = file.path(plot_path, "lm_results_questionnaires_emmeans_2025-09-05.csv"),  row.names = FALSE)


# Filter results for group comparison
anova_results_df_questionnaires_total_groupSZ <- anova_results_df_questionnaires_total[
  anova_results_df_questionnaires_total$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_questionnaires_total_groupSZ$p_adjusted <- p.adjust(anova_results_df_questionnaires_total_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_questionnaires_total_groupSZ)

# SAVE !!!
# Save the data frame as a CSV file to the directory
write.csv(anova_results_df_questionnaires_total_groupSZ, file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-09-05.csv"), row.names = FALSE)

```

TEST: almost same as above but has CI and BFs
BF
```{r}
library(emmeans)
library(car)
library(lsr)         # for etaSquared
library(BayesFactor)
library(dplyr)
library(ggplot2)

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires"

# Initialize lists
combined_results <- list()
combined_emms <- list()
combined_bf <- list()

for (var in questionnaire_vars_totals) {

  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  # Extract coefficients
  summary_lm <- summary(lm_result)
  coef_df <- as.data.frame(summary_lm$coefficients)
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- var
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Add 95% CI
  ci_df <- as.data.frame(confint(lm_result))
  ci_df$predictor <- rownames(ci_df)
  rownames(ci_df) <- NULL
  colnames(ci_df) <- c("CI_lower", "CI_upper", "predictor")
  coef_df <- merge(coef_df, ci_df, by = "predictor")
  
  # Calculate eta-squared
  effect_size <- etaSquared(lm_result)
  effect_size_df <- as.data.frame(effect_size)
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- var
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  # Merge coefficients with effect size
  combined_table <- full_join(coef_df, effect_size_df, by = c("predictor", "outcome"))
  combined_results[[var]] <- combined_table
  
  # Compute emmeans for group
  emm <- emmeans(lm_result, ~ group)
  emm_df <- as.data.frame(emm)
  emm_df$predictor <- rownames(emm_df)
  rownames(emm_df) <- NULL
  emm_df$outcome <- var
  combined_emms[[var]] <- emm_df
  
  # Compute Bayes Factors
  # Note: lmBF sometimes fails if there is only 1 level for a factor in the subset
  bf_result <- tryCatch(
    lmBF(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")),
         data = questionnaire_data),
    error = function(e) NA
  )
  
  if (!is.na(bf_result)) {
    bf_df <- as.data.frame(bf_result)
    bf_df$predictor <- rownames(bf_df)
    rownames(bf_df) <- NULL
    bf_df$outcome <- var
    combined_bf[[var]] <- bf_df
  }

}

# Combine all results
anova_results_df_questionnaires_total <- do.call(rbind, combined_results)
anova_results_emms_questionnaires_total <- do.call(rbind, combined_emms)
anova_results_bf_questionnaires_total <- do.call(rbind, combined_bf)

```


```{r}
# BAYESIAN LINEAR MIXED MODELS: exercise effects on structural and functional brain patterns  -------------------------------------------------------------------------------------------------------------

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/questionnaires"

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index")
)

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0"
)

# initialize empty list
combined_results_bf <- list()

#Loop
for (var in questionnaire_vars_totals) { # or questionnaire_vars

  # Define the model formula
  formula <- as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index + (1 | subject)"))
  
  # Fit the Bayesian model
  bayes_model <- brm(formula = formula, data = questionnaire_data, prior = priors, sample_prior = TRUE, chains = 4, cores = 4, iter = 4000, seed = 123)
  
  # Summary of the Bayesian model
  summary(bayes_model)
  
  # Extract posterior estimates
  posterior_summary <- posterior_summary(bayes_model)
  
  # Compute model fit statistics like WAIC or LOO
  waic_result <- waic(bayes_model)
  #loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?
  
  ## H1 results
  hypothesis_results <- hypothesis(bayes_model, hypotheses)
  df_hypothesis <- data.frame(hypothesis_results$hypothesis)
  
  # Calculate Bayes Factors (BF10)
  df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
  df_hypothesis <- df_hypothesis %>%
    rename(
      b_estimate = Estimate,
      err = Est.Error,
      CI.low = CI.Lower,
      CI.high = CI.Upper,
      evid.ratio = Evid.Ratio,
      post.prob = Post.Prob,
      sig = Star
    ) %>%
    mutate(outcome = var) 
  
  # Store the combined results in the list
  combined_results_bf[[var]] <- df_hypothesis
}

# Combine all results into one dataframe
bf_lm_results_df_questionnaires <- do.call(rbind, combined_results_bf)
# save 
write.csv(bf_lm_results_df_questionnaires, file = file.path(plot_path, "bf_lm_results_main_questionnaires_2025-05-06.csv"), row.names = FALSE)
```

TEST All BPQs:

```{r lm for loop: BPQs}

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires"

# Initialize a list to store the results
combined_results_bpq <- list()

for (var in BPQ_vars_no_total) {
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  #### LMEM ####
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (std-beta)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate_beta = Std_Coefficient,
    std_beta_95CI_low = CI_low,
    std_beta_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = var) %>%
  select(predictor, outcome, std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)

  # Merge the tables on the predictor variable
  combined_table <- full_join(std_beta_df, coef_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results_bpq[[var]] <- combined_table
  
}

# Combine all results into one dataframe
anova_results_df_bpq_questionnaires <- do.call(rbind, combined_results_bpq)
# anova_results_df_bpq_questionnaires_adjusted_for_all_covars <- anova_results_df_bpq_questionnaires
# make a df only for group comparison
# anova_results_df_bpq_questionnaires_group <- do.call(rbind, combined_results_group)

# get only results of interest
# Filter the results where the predictor "groupSZ"
bpq_results_of_interest <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("groupSSD"), 
]
bpq_results_of_interest_group <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("group"), 
]

# Correct for multiple comparisons using the Benjamini-Hochberg procedure
bpq_results_of_interest$p_adjusted <- p.adjust(bpq_results_of_interest$p_value, method = "BH",  n = length(BPQ_vars_no_total)) #  n = length(BPQ_vars)  
#bpq_results_of_interest_group$p_adjusted <- p.adjust(bpq_results_of_interest_group$p, method = "BH",  n = length(BPQ_vars_no_total)) #  n = length(BPQ_vars)  
# https://stackoverflow.com/questions/30108510/p-adjust-with-n-than-number-of-tests
# https://stats.stackexchange.com/questions/417234/how-to-correct-p-values-of-two-multiple-regression-models
# anova_results_df_bpq_questionnaires_adjusted_for_all_covars$p_adjusted <- p.adjust(anova_results_df_bpq_questionnaires_adjusted_for_all_covars$p, method = "BH")

# Format p-values to avoid scientific notation
bpq_results_of_interest$p_adjusted <- format(bpq_results_of_interest$p_adjusted, scientific = FALSE) # ,  digits = 3
# anova_results_df_bpq_questionnaires_adjusted_for_all_covars$p_adjusted <- format(anova_results_df_bpq_questionnaires_adjusted_for_all_covars$p_adjusted, scientific = FALSE)  # , digits = 3

# View the results
print(bpq_results_of_interest)

# SAVE !!!
write.csv(bpq_results_of_interest, file = file.path(plot_path, "lm_results_BPQ-subscales_groupSZ_p-adjusted_2025-09-05.csv"),row.names = FALSE)

#write.csv(anova_results_df_bpq_questionnaires_adjusted_for_all_covars, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_BPQ_questionnaires_p-adjusted_for_all_covars.csv", row.names = FALSE)

#write.csv(bpq_results_of_interest, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_BPQ_questionnaires_p-adjusted_for_all_hypotheses.csv", row.names = FALSE)


```

BF

```{r}
# BAYESIAN LINEAR MIXED MODELS: -------------------------------------------------------------------------------------------------------------

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/questionnaires"

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index")
)

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0"
)

# initialize empty list
combined_results_bf <- list()

#Loop
for (var in BPQ_vars_no_total) { # or questionnaire_vars

  # Define the model formula
  formula <- as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index + (1 | subject)"))
  
  # Fit the Bayesian model
  bayes_model <- brm(formula = formula, data = questionnaire_data, prior = priors, sample_prior = TRUE, chains = 4, cores = 4, iter = 4000, seed = 123)
  
  # Summary of the Bayesian model
  summary(bayes_model)
  
  # Extract posterior estimates
  posterior_summary <- posterior_summary(bayes_model)
  
  # Compute model fit statistics like WAIC or LOO
  waic_result <- waic(bayes_model)
  #loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?
  
  ## H1 results
  hypothesis_results <- hypothesis(bayes_model, hypotheses)
  df_hypothesis <- data.frame(hypothesis_results$hypothesis)
  
  # Calculate Bayes Factors (BF10)
  df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
  df_hypothesis <- df_hypothesis %>%
    rename(
      b_estimate = Estimate,
      err = Est.Error,
      CI.low = CI.Lower,
      CI.high = CI.Upper,
      evid.ratio = Evid.Ratio,
      post.prob = Post.Prob,
      sig = Star
    ) %>%
    mutate(outcome = var) 
  
  # Store the combined results in the list
  combined_results_bf[[var]] <- df_hypothesis
}

# Combine all results into one dataframe
bf_lm_results_df_BPQ_subscales <- do.call(rbind, combined_results_bf)
# save 
write.csv(bf_lm_results_df_BPQ_subscales, file = file.path(plot_path, "bf_lm_results_BPQ_subscales_2025-05-06.csv"), row.names = FALSE)
```

TEST All MAIAs

```{r lm for loop: MAIAs}

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/Questionnaires"

# Initialize a list to store the results
combined_results_maia <- list()

for (var in MAIA_vars_no_total) {
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  #### LMEM ####
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (std-beta)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate_beta = Std_Coefficient,
    std_beta_95CI_low = CI_low,
    std_beta_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = var) %>%
  select(predictor, outcome, std_estimate_beta, std_beta_95CI_low, std_beta_95CI_high)

  # Merge the tables on the predictor variable
  combined_table <- full_join(std_beta_df, coef_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results_maia[[var]] <- combined_table
  
}

# Combine all results into one dataframe
anova_results_df_maia_questionnaires <- do.call(rbind, combined_results_maia)

#### Option 1: adjust for all covars:
#anova_results_df_maia_questionnaires_adjusted_for_all_covars <- anova_results_df_maia_questionnaires
#anova_results_df_maia_questionnaires_adjusted_for_all_covars$p_adjusted <- p.adjust(anova_results_df_maia_questionnaires_adjusted_for_all_covars$p, method = "BH")
# Save !!!
#write.csv(anova_results_df_maia_questionnaires_adjusted_for_all_covars, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_MAIA_questionnaires_p-adjusted_for_all_covars.csv", row.names = FALSE)


#### Option 2: adjust only per hypothesis / questionnaire / get only results of interest YES...

# get only results of interest

# Filter the results where the predictor is either "group" or "groupSZ"
maia_results_of_interest <- anova_results_df_maia_questionnaires[
  anova_results_df_maia_questionnaires$predictor %in% c("groupSSD"), 
]
maia_results_of_interest_group <- anova_results_df_maia_questionnaires[
  anova_results_df_maia_questionnaires$predictor %in% c("group"), 
]
# Correct for multiple comparisons using the Benjamini-Hochberg procedure
maia_results_of_interest$p_adjusted <- p.adjust(maia_results_of_interest$p_value, method = "BH",  n = length(MAIA_vars_no_total)) #  n = length(BPQ_vars)  
# https://stackoverflow.com/questions/30108510/p-adjust-with-n-than-number-of-tests
# https://stats.stackexchange.com/questions/417234/how-to-correct-p-values-of-two-multiple-regression-models
maia_results_of_interest$p_adjusted <- format(maia_results_of_interest$p_adjusted, scientific = FALSE) # ,  digits = 3


print(maia_results_of_interest)

write.csv(maia_results_of_interest, file = file.path(plot_path, "lm_results_MAIA-subscales_group_p-adjusted_2025-09-05.csv"),row.names = FALSE)
# save
#write.csv(maia_results_of_interest, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_MAIA_questionnaires_p-adjusted_for_all_hypotheses.csv", row.names = FALSE)


```

BF

```{r}
# BAYESIAN LINEAR MIXED MODELS: exercise effects on structural and functional brain patterns  -------------------------------------------------------------------------------------------------------------

plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/questionnaires"

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index")
)

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0"
)

# initialize empty list
combined_results_bf <- list()

#Loop
for (var in MAIA_vars_no_total) { # or questionnaire_vars

  # Define the model formula
  formula <- as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index + (1 | subject)"))
  
  # Fit the Bayesian model
  bayes_model <- brm(formula = formula, data = questionnaire_data, prior = priors, sample_prior = TRUE, chains = 4, cores = 4, iter = 4000, seed = 123)
  
  # Summary of the Bayesian model
  summary(bayes_model)
  
  # Extract posterior estimates
  posterior_summary <- posterior_summary(bayes_model)
  
  # Compute model fit statistics like WAIC or LOO
  waic_result <- waic(bayes_model)
  #loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?
  
  ## H1 results
  hypothesis_results <- hypothesis(bayes_model, hypotheses)
  df_hypothesis <- data.frame(hypothesis_results$hypothesis)
  
  # Calculate Bayes Factors (BF10)
  df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
  df_hypothesis <- df_hypothesis %>%
    rename(
      b_estimate = Estimate,
      err = Est.Error,
      CI.low = CI.Lower,
      CI.high = CI.Upper,
      evid.ratio = Evid.Ratio,
      post.prob = Post.Prob,
      sig = Star
    ) %>%
    mutate(outcome = var) 
  
  # Store the combined results in the list
  combined_results_bf[[var]] <- df_hypothesis
}

# Combine all results into one dataframe
bf_lm_results_df_MAIA_subscales <- do.call(rbind, combined_results_bf)
# save 
write.csv(bf_lm_results_df_MAIA_subscales, file = file.path(plot_path, "bf_lm_results_MAIA_subscales_2025-05-06.csv"), row.names = FALSE)
```

TEST CDS

```{r CDS}
# compute LMM and save statistics of main effect as data frame
lm_result <- lm(CDS ~ group +age + sex + education_years + body_mass_index, 
               data=questionnaire_data,
               na.action=na.omit)


# get the model fits:  AIC (Akaike Information Criterion), or BIC (Bayesian Information Criterion)
#AIC(lm_result)
#BIC(lm_result)

# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)

# anova() Computes analysis of variance (or deviance) tables for one or more fitted model objects.
anova_table_cds <- anova(lm_result)

# Add predictor and outcome columns
anova_table_cds$predictor <- rownames(anova_table_cds)
rownames(anova_table_cds) <- NULL # now we have a col for predictors, dont need the rownames
anova_table_cds$outcome <- rep("CDS", times = nrow(anova_table_cds))
# df_den
n <- nobs(lm_result)
k <- length(levels(questionnaire_data$group))
df_den = n - k
anova_table_cds$df_den <- df_den

# Rename the columns appropriately
colnames(anova_table_cds) = c("df_num","sum_sq","mean_sq","f","p","predictor","outcome", "df_den")

# Reorder the columns
anova_table_cds = anova_table_cds[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

# Format p-values to avoid scientific notation
anova_table_cds$p <- format(anova_table_cds$p, scientific = FALSE, digits = 3)

# Correct for multiple comparisons using the Benjamini-Hochberg procedure, for 3 families of quuestionnaires, bpq, maia, cds
anova_table_cds_corrected <- anova_table_cds[
  anova_table_cds$predictor %in% c("group"), 
]
nr_of_test_fams = 3
anova_table_cds_corrected$p_adjusted <- p.adjust(anova_table_cds_corrected$p, method = "BH", n = nr_of_test_fams ) # , 

# SAVE
write.csv(anova_table_cds, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_CDS_not_adjusted.csv", row.names = FALSE)

write.csv(anova_table_cds_corrected, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_CDS_p-adjusted_for_all_3-tests.csv", row.names = FALSE)


```

#### PLOTS

PLOT BPQ_total

```{r plot single}

# Create a violin plot of BPQ_total by group
ggplot(questionnaire_data, aes(x = group, y = BPQ_total, fill = group)) +
  geom_violin(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Distribution of BPQ_total by Group", 
       x = "Group", 
       y = "BPQ_total") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

# Create a boxplot of BPQ total by group
bpq_total_boxplot <- ggplot(questionnaire_data, aes(x = group, y = BPQ_total, fill = group)) +
  geom_boxplot() +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Distribution of Body Perception Questionnaire Results by Group", 
       x = "Group", 
       y = "BPQ Total") +
  theme_minimal() +
  theme ( legend.position = c(0.9, 0.8),
          legend.text = element_text(size = 18),  # Increase legend text size
          legend.title = element_text(size = 18), # Increase legend title size
          legend.key.size = unit(1.5, "cm"), 
          plot.title = element_text(size = 12, face = "bold"),  # Title size and bold
          axis.title = element_text(size = 18),   # Axis title size
          axis.text = element_text(size = 14),    # Axis text size
          strip.text = element_text(size = 16)  ) +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

bpq_total_boxplot

# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "BPQ_total_boxplot_by_group_raw.png"), plot = bpq_total_boxplot, width = 8, height = 6, dpi = 300)





```

PLOT MAIA_total

```{r plot single}

# Create a violin plot of BPQ_total by group
ggplot(questionnaire_data, aes(x = group, y = MAIA_total, fill = group)) +
  geom_violin(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Distribution of MAIA_total by Group", 
       x = "Group", 
       y = "MAIA_total") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme


# Create a boxplot of MAIA_total by group
maia_total_boxplot <- ggplot(questionnaire_data, aes(x = group, y = MAIA_total, fill = group)) +
  geom_boxplot(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Distribution of Multidimensional Assessment of Interoceptive Awareness Results by Group", 
       x = "Group", 
       y = "MAIA Total") +
  theme_minimal() +
  theme ( legend.position = c(0.9, 0.8),
          legend.text = element_text(size = 18),  # Increase legend text size
          legend.title = element_text(size = 18), # Increase legend title size
          legend.key.size = unit(1.5, "cm"), 
          plot.title = element_text(size = 12, face = "bold"),  # Title size and bold
          axis.title = element_text(size = 18),   # Axis title size
          axis.text = element_text(size = 14),    # Axis text size
          strip.text = element_text(size = 16)  ) +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

maia_total_boxplot


# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "MAIA_total_boxplot_by_group_raw.png"),
         plot = maia_total_boxplot,
         width = 8, height = 6, dpi = 300)


#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Sensibility/MBB_results/maia_total_boxplot.jpg", plot = maia_total_boxplot, width = 12, height = 12, dpi = 500)



```

PLOT Boxplot ALL BPQ

```{r}

# Set color palette
color_palette <- brewer.pal(n = 8, name = "Set3")

# Define function to create box plots
create_box_plot <- function(questionnaire_data, y_var, color_palette) {
  ggplot(questionnaire_data, aes(x = group, y = questionnaire_data[[y_var]], fill = group)) +
    geom_boxplot(outlier.colour = "black", outlier.shape = 16) +
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +
    scale_fill_manual(values = color_palette) +
    labs(title = y_var, x = "Group", y = "Value") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(size = 12)
    )
}

# Create plots for each variable
p1 <- create_box_plot(questionnaire_data, "BPQ_body_awareness", color_palette)
p2 <- create_box_plot(questionnaire_data, "BPQ_autonomic", color_palette)
p3 <- create_box_plot(questionnaire_data, "BPQ_sub_diaphragmatic", color_palette)
p4 <- create_box_plot(questionnaire_data, "BPQ_supra_diaphragmatic", color_palette)
p5 <- create_box_plot(questionnaire_data, "BPQ_total", color_palette)

# Combine the plots into a grid
bpq_boxplots <- (p1 | p2 | p3) / (p4 | p5)

print(bpq_boxplots)

ggsave(filename = paste0(plot_path, "BPQ_all_boxplot_by_group_raw.png"),
         plot = bpq_boxplots,
         width = 8, height = 6, dpi = 300)


```

PLOT Boxplot MAIA & BPQ ALL

```{r plot fun}
# Set color palette
color_palette <- brewer.pal(n = 8, name = "Set3")

# Define function to create box plots
create_box_plot <- function(questionnaire_data, y_var, color_palette) {
  ggplot(questionnaire_data, aes(x = group, y = questionnaire_data[[y_var]], fill = group)) +
    geom_boxplot(outlier.colour = "black", outlier.shape = 16) +
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +
    scale_fill_manual(values = color_palette) +
    labs(title = y_var, x = "Group", y = "Value") +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(size = 12)
    )
}

# Create plots for each variable
p1 <- create_box_plot(questionnaire_data, "MAIA_noticing", color_palette)
p2 <- create_box_plot(questionnaire_data, "MAIA_not_distracting", color_palette)
p3 <- create_box_plot(questionnaire_data, "MAIA_not_worrying", color_palette)
p4 <- create_box_plot(questionnaire_data, "MAIA_attention_regulation", color_palette)
p5 <- create_box_plot(questionnaire_data, "MAIA_emotional_awareness", color_palette)
p6 <- create_box_plot(questionnaire_data, "MAIA_self_regulation", color_palette)
p7 <- create_box_plot(questionnaire_data, "MAIA_body_listening", color_palette)
p8 <- create_box_plot(questionnaire_data, "MAIA_trusting", color_palette)
p9 <- create_box_plot(questionnaire_data, "MAIA_total", color_palette)

# Combine the plots into a grid
maia_boxplot <- (p1 | p2 | p3) / (p4 | p5 | p6) / (p7 | p8 | p9)

# Print the combined plot
print(maia_boxplot)

# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "MAIA-subscales_boxplot_by_group_raw.png"),
         plot = maia_boxplot,
         width = 8, height = 6, dpi = 300)

### Now BPQ
# Create plots for each variable
p1 <- create_box_plot(questionnaire_data, "BPQ_body_awareness", color_palette)
p2 <- create_box_plot(questionnaire_data, "BPQ_sub_diaphragmatic", color_palette)
p3 <- create_box_plot(questionnaire_data, "BPQ_supra_diaphragmatic", color_palette)

# Combine the plots into a grid
bpq_boxplot <- (p1 | p2 | p3) 

# Print the combined plot
print(bpq_boxplot)

# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "BPQ-subscales_boxplot_by_group_raw.png"),
         plot = bpq_boxplot,
         width = 8, height = 6, dpi = 300)

#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/maia_boxplots.png", plot = maia_boxplot, width = 12, height = 12, dpi = 300)

```

PLOT CDS

```{r}

# Create a violin plot of BPQ_total by group
cds_plot <- ggplot(questionnaire_data, aes(x = group, y = CDS, fill = group)) +
  geom_boxplot(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Distribution of Depersonalization by Group", 
       x = "Group", 
       y = "CDS") +
  theme_minimal() +
  theme ( legend.position = c(0.9, 0.8),
          legend.text = element_text(size = 18),  # Increase legend text size
          legend.title = element_text(size = 18), # Increase legend title size
          legend.key.size = unit(1.5, "cm"), 
          plot.title = element_text(size = 12, face = "bold"),  # Title size and bold
          axis.title = element_text(size = 18),   # Axis title size
          axis.text = element_text(size = 14),    # Axis text size
          strip.text = element_text(size = 16)  ) +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

cds_plot

# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "CDS_boxplot_by_group_raw.png"),
         plot = cds_plot,
         width = 8, height = 6, dpi = 300)

#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Sensibility/MBB_results/cds_boxplot.jpg", plot = cds_plot, width = 12, height = 12, dpi = 300)


```

Archive:

TEST All questionnaires: HERE how to correct p? Per hypothesis (questionnaire = 3) or per subscale of questionnaire (subscales = 13) or per test (covariate = wy too many =\> NO)?

```{r lm for loop. including all quesionnaires}


# Initialize a list to store the results
combined_results <- list()
combined_results_group <-  list()

for (var in questionnaire_vars_no_totals) { # or questionnaire_vars
  
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = questionnaire_data,
                  na.action = na.omit)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  combined_table <- merge(anova_table, coef_df, by = c("outcome", "predictor"), all = TRUE)
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
  
  
}


# Combine all results into one dataframe
anova_results_df_questionnaires <- do.call(rbind, combined_results)

# make a df only for group comparison, get only results of interest Filter the results where the predictor "groupSZ" and "group"
anova_results_df_questionnaires_groupSZ <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("groupSSD"), 
]
anova_results_df_questionnaires_group <- anova_results_df_bpq_questionnaires[
  anova_results_df_bpq_questionnaires$predictor %in% c("group"), 
]


# Correct for multiple comparisons using the Benjamini-Hochberg procedure
# Calculate the total number of tests (number of subscales in all questionnaires)
# n_tests <- sum(c(4, 8, 1))  # 4 from Q1, 8 from Q2, 1 from Q3
anova_results_df_questionnaires$p_adjusted <- p.adjust(anova_results_df_questionnaires$p, method = "BH") #  n = n_tests # add n_length!! https://stackoverflow.com/questions/30108510/p-adjust-with-n-than-number-of-tests

anova_results_df_questionnaires_groupSZ$p_adjusted <- p.adjust(anova_results_df_questionnaires_groupSZ$p_value, method = "BH",  n = length(BPQ_vars)) #  n = length(BPQ_vars)  
anova_results_df_questionnaires_group$p_adjusted <- p.adjust(anova_results_df_questionnaires_group$p, method = "BH",  n = length(BPQ_vars)) #  n = length(BPQ_vars)  


# Format p-values to avoid scientific notation
anova_results_df_questionnaires$p_adjusted <- format(anova_results_df_questionnaires$p_adjusted, scientific = FALSE, digits = 3)
anova_results_df_questionnaires_group$p_adjusted <- format(anova_results_df_questionnaires_group$p_adjusted, scientific = FALSE, digits = 3)
anova_results_df_questionnaires_groupSZ$p_adjusted <- format(anova_results_df_questionnaires_groupSZ$p_adjusted, scientific = FALSE, digits = 3)


# View the results
print(anova_results_df_questionnaires)
print(anova_results_df_questionnaires_group)
print(anova_results_df_questionnaires_groupSZ)

# First, make sure to convert p_adjusted into a numeric format
#anova_results_df_questionnaires_group$p_adjusted <- as.numeric(anova_results_df_questionnaires_group$p_adjusted)

# Now, format it to avoid scientific notation and make it into a string
#anova_results_df_questionnaires_group$p_adjusted <- format(anova_results_df_questionnaires_group$p_adjusted,  scientific = FALSE, digits = 3)

# SAVE !!!
write.csv(anova_results_df_questionnaires, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_lm_results_questionnaires_p-adjusted_2025-02-20.csv", row.names = FALSE)
write.csv(anova_results_df_questionnaires_group, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/anova_results_questionnaires_group_p-adjusted_2025-02-20.csv", row.names = FALSE)
write.csv(anova_results_df_questionnaires_groupSZ, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/lm_results_questionnaires_group_p-adjusted_2025-02-20.csv", row.names = FALSE)




```

Try to add the effect size with the code below:

```{r}
# Calculate the effect size (eta-squared)
effect_size <- etaSquared(lm_result)
effect_size_df <- as.data.frame(effect_size)
# Add predictor and outcome columns to the coefficients data frame
effect_size_df$predictor <- rownames(effect_size_df)
rownames(effect_size_df) <- NULL
effect_size_df$outcome <- rep("interoceptive_accuracy", times = nrow(effect_size_df))

# Rename coefficient columns for clarity
colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")


# Merge the ANOVA and coefficient tables on the predictor variable
combined_table_interoceptive_accuracy_eta <- full_join(combined_table_interoceptive_accuracy, effect_size_df, by = c("outcome", "predictor"))
```

```{r Violin plot for residuals}
# Identify rows used in the linear model
used_rows <- !is.na(questionnaire_data$group) & 
              complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])

# Create a new dataframe with residuals and corresponding groups
residuals_data <- data.frame(
  group = questionnaire_data$group[used_rows],
  residuals = residuals(lm_result)
)

# Step 2: Create a violin plot of residuals by group
ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
  geom_violin(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = paste("Distribution of Residuals by Group for", var), 
       x = "Group", 
       y = "Residuals") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

```

```{r Boxplot for residuals}
# Identify rows used in the linear model
used_rows <- !is.na(questionnaire_data$group) & 
              complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])

# Create a new dataframe with residuals and corresponding groups
residuals_data <- data.frame(
  group = questionnaire_data$group[used_rows],
  residuals = residuals(lm_result)
)

# Step 2: Create a boxplot of residuals by group
ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Boxplot without separate outlier points
  geom_jitter(width = 0.2, alpha = 0.5, color = "black") +  # Show individual data points
  labs(title = paste("Distribution of Residuals by Group for", var), 
       x = "Group", 
       y = "Residuals") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

```

PLOT VIOLIN BPQ ALL

```{r plot manual}

# Create individual violin plots
p1 <- ggplot(questionnaire_data, aes(x = group, y = BPQ_body_awareness, fill = group)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "BPQ Body Awareness", x = "Group", y = "Value") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")

p2 <- ggplot(questionnaire_data, aes(x = group, y = BPQ_autonomic, fill = group)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "BPQ Autonomic", x = "Group", y = "Value") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")

p3 <- ggplot(questionnaire_data, aes(x = group, y = BPQ_sub_diaphragmatic, fill = group)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "BPQ Sub Diaphragmatic", x = "Group", y = "Value") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")

p4 <- ggplot(questionnaire_data, aes(x = group, y = BPQ_supra_diaphragmatic, fill = group)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "BPQ Supra Diaphragmatic", x = "Group", y = "Value") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")

p5 <- ggplot(questionnaire_data, aes(x = group, y = BPQ_total, fill = group)) +
  geom_violin(trim = FALSE) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "BPQ Total", x = "Group", y = "Value") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3")

# Combine the plots into a grid
# grid.arrange(p1, p2, p3, p4, p5, ncol = 3)

# Combine the plots into a grid
(p1 | p2 | p3) / (p4 | p5)

```

PLOT VIOLIN MAIA ALL

```{r plot fun}

# Set color palette
color_palette <- brewer.pal(n = 8, name = "Set3")

# Define function to create violin plots
create_violin_plot <- function(questionnaire_data, y_var, color_palette) {
  ggplot(questionnaire_data, aes(x = group, y = questionnaire_data[[y_var]], fill = group)) +
    geom_violin(trim = FALSE) +
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +
    scale_fill_manual(values = color_palette) +
    labs(title = y_var, x = "Group", y = "Value") +
    theme_minimal() +
    theme(legend.position = "none",
    plot.title = element_text(size = 16))
}

# Create plots for each variable
p1 <- create_violin_plot(questionnaire_data, "MAIA_noticing", color_palette)
p2 <- create_violin_plot(questionnaire_data, "MAIA_not_distracting", color_palette)
p3 <- create_violin_plot(questionnaire_data, "MAIA_not_worrying", color_palette)
p4 <- create_violin_plot(questionnaire_data, "MAIA_attention_regulation", color_palette)
p5 <- create_violin_plot(questionnaire_data, "MAIA_emotional_awareness", color_palette)
p6 <- create_violin_plot(questionnaire_data, "MAIA_self_regulation", color_palette)
p7 <- create_violin_plot(questionnaire_data, "MAIA_body_listening", color_palette)
p8 <- create_violin_plot(questionnaire_data, "MAIA_trusting", color_palette)
p9 <- create_violin_plot(questionnaire_data, "MAIA_total", color_palette)

# Combine the plots into a grid
maia_plot <- (p1 | p2 | p3) / (p4 | p5 | p6) / (p7 | p8 | p9)

# Print the combined plot
print(maia_plot)


# make a dummy plot to extract the legend
# Create a dummy plot with legend
legend_plot <- ggplot(questionnaire_data, aes(x = group, y = MAIA_noticing, fill = group)) +
  geom_violin(trim = FALSE) +
  scale_fill_manual(values = color_palette) +
  theme_minimal() +
  labs(fill = "Group")

legend_plot

#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/maia_plots.png", plot = maia_plot, width = 12, height = 12, dpi = 300)
#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/maia_plots_for_legend.png", plot = legend_plot, width = 12, height = 12, dpi = 300)


```

### 1.2) Interoceptive accuracy is reduced in patients as measured by the Heartbeat Counting Task (HCT). (directional hypothesis)

We will build a regression model for each hypothesis, where the variables of interest (Interoceptive Sensibility and Interoceptive Accuracy data) will be the dependent variable and the diagnostic group (HC vs. SSD) will be the predictor, along with the covariates of age, gender, BMI, and years of education. For the Interoceptive Accuracy model, we will also add Heart Rate (HR), smoking, caffeine intake, knowledge of heart rate, and the Not-Worrying subscale of MAIA as a proxy for anxiety about bodily sensations as covariates.

#### Check for assumptions

Check for Assumptions for lm() (Linear Regression) 1. Linearity The relationship between predictors and the outcome variable is linear. 2. Independence of Observations Observations are independent of each other. 3. Homoscedasticity The residuals have constant variance across all levels of the predictors. 4. Normality of Residuals The residuals are normally distributed. 5. No Multicollinearity Predictors are not highly correlated with each other. 6. No Outliers or High Leverage Points There are no extreme outliers or influential points that unduly affect the model.

Define the model

```{r}
# compute LM
lm_result <- lm(interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
               data=iacc_data,
               na.action=na.exclude)

lm_log_result <- lm(log_interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
               data=iacc_data,
               na.action=na.exclude)

### TRY BAYESIAN
# compute Bayesian LM and save statistics of main effect as data frame
iacc_data_no_na <- iacc_data %>%
  select(subject, interoceptive_accuracy, group, age, sex, education_years, body_mass_index, heart_rate_bpm, smoker, had_caffeine, knows_heartrate, MAIA_not_worrying) %>%
  na.omit()

lm_result_bf <- lmBF(interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
               data=iacc_data_no_na,
               na.action=na.omit)

lmer_model <- lmer(interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying + (1 | subject), 
               data=iacc_data_no_na)

```

##### 1. Linearity =\> Some folding in the middle...

```{r}

#plot: shows the relationship between each predictor and the residuals, adjusted for the other predictors in the model.
crPlots(lm_result) 

# you can also plot residuals vs predictions
#If the plot shows random scatter around 0 without any pattern, it suggests linearity. If there's a clear curve, the linearity assumption might be violated.
plot(lm_result$fitted.values, lm_result$residuals)
abline(h = 0, col = "red")

# or same thing but different..plot Residuals vs Fitted
plot(lm_result, which = 1, main = paste("Residuals vs Fitted for IAcc"))



```

If violated, for example, let's say that interoceptive_accuracy is non-linear, showing exponential growth. We can linearize it by a log-transformation.

```{r}
# You can consider a log transformation
beh_data$log_interoceptive_accuracy <- log(beh_data$interoceptive_accuracy)
iacc_data$log_interoceptive_accuracy <- log(iacc_data$interoceptive_accuracy)


```

If you do this, do not forget to update your model with the new variable!

##### 2. Independence of Observations (Independence of Residuals?) =\> SATISFIED

If the residuals are autocorrelated, this can indicate dependence of observations, hence a violation. You can check residual autocorrelation using the Durbin-Watson test, which tests for autocorrelation in residuals. If the p-value is significant, it suggests that the assumption of independence might be violated.

```{r}

dw_test <- dwtest(lm_result)
print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))

```

If violated, you can use the gls() function from the nlme package to define a new Generalized Least Squares (GLS) model.

##### 3. Homoscedasticity (Constant Variance of Residuals) =\> Test not sig but some fold on red line

If the plot shows a funnel shape (residuals increasing or decreasing as fitted values increase), or if the Breusch-Pagan test gives a significant result, the assumption of homoscedasticity may be violated.

```{r}

# Visual check
plot(lm_result$fitted.values, lm_result$residuals)
abline(h = 0, col = "red")
# test
bptest(lm_result) 

# Scale-Location Plot
plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))


```

Breusch-Pagan Test: testing for homoscedasticity: p \< 0.05: Evidence of heteroscedasticity, consider transforming the dependent variable, like we did under linearity.

##### 4. Normality of Residuals =\> SATISFIED

If the Q-Q plot shows significant deviation from the line, or if the Shapiro-Wilk test returns a significant result (p-value \< 0.05), the normality assumption may be violated.

```{r}

# Q-Q plot
qqnorm(lm_result$residuals)
qqline(lm_result$residuals, col = "red")
# test
shapiro.test(residuals(lm_result)) # p < 0.05 => Residuals deviate from normality


plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))


```

If so, GLS and transformation of the dependent variable can be used again.

##### 5. No Multicollinearity. =\> SATISFIED

You can use the Variance Inflation Factor (VIF) to check for multicollinearity.

```{r}
vif_values <- vif(lm_result)
print(paste("VIF for predictors in", var, ":"))
print(vif_values)
```

VIF \> 5: Considered problematic multicollinearity.; VIF \> 10: Very severe multicollinearity, drop or combine predictors. E.g. You can run a principle component analysis (PCA) to reduce the dimensionalit of the predictors and only keep uncorrelated variables. skip this here:

```{r }
# Do PCA on correlated vars, let's assume age and education years were highly correlated
correlated_vars <- beh_data[, c("age", "education_years")]
correlated_vars_scaled <- scale(correlated_vars)
pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")  # Scree plot
beh_data$PC1 <- pca_result$x[, 1]  # First principal component
beh_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

# Now you should change the age and education years with the PC1 (and PC2 if enough variance is explained)
```

##### 6. No Outliers or High Leverage Points =\> SATISFIED

If you see points with Cook’s distance greater than 1 or leverage values much higher than the average (typically above 2 times the average leverage), they may be outliers or influential points.

```{r}

# Cook's distance
plot(lm_result, which = 4)
# Leverage statistics
hatvalues(lm_result)
plot(hatvalues(lm_result), main = "Leverage Values", ylab = "Leverage", xlab = "Observation Index")
abline(h = threshold, col = "red", lty = 2)  # Add the threshold as a reference line

# Outlier Test
outlierTest(lm_result) # # If the test returns "No Studentized residuals with Bonferroni p < 0.05", it suggests there are no significant outliers.

```

If not satisfied: You can remove the outliers or transfer the relevant variable. Be careful with removing data points though: Make sure that the removal of outliers does not introduce bias or affect the representativeness of your sample. Removing too many data points could distort the generalization of your findings. Sometimes outliers can still be meaningful data, " feature, not a bug", especially in clinical data. You need to make a trade-off assessment and a theoretically informed decision. Another option would be capping the outliers to a certain percentile.

Skip: E.g.:

```{r}
# Get Cook's distance and identify influential points
cooks_distance <- cooks.distance(lm_result)
influential_points <- which(cooks_distance > 1)  # points with Cook's distance > 1

# Alternatively, use the Outlier Test's significant points
outliers <- rownames(outlier_test$res)  # Get the row names of significant outliers

# Remove the identified outliers from your dataset (you can do it based on either test)
beh_data_clean <- beh_data[-c(influential_points, outliers), ]

```

##### 7. Additional diagnostic tests: Model specification test

The RESET test (Regression Specification Error Test) checks whether the model is properly specified. If the model has misspecified relationships (e.g., omitted variables or nonlinear relationships), the RESET test can indicate this. =\> p \< 0.05: Model might be misspecified (e.g., nonlinear relationships, omitted predictors).

```{r}

resettest(lm_result) 

```

If the p-value is less than 0.05, it suggests that the model might be misspecified, meaning the linearity assumption could be violated (nonlinear relationships, omitted variables). To address this, check for non-linear relatinships, check whether more predictors or interactions may be needed in the model, or try alternative/non-linear model types.

##### IF assumptions are not satisfied =\> change the model as needed and re-define the model

Log-trans: 

```{r}
# You can consider a log transformation
#iacc_data$log_interoceptive_accuracy <- log(iacc_data$interoceptive_accuracy)
iacc_data$log_interoceptive_accuracy <- log(iacc_data$interoceptive_accuracy + 0.0001) # Do this because zeros cause inf values which cause errors later !!!


# compute LMM and save statistics of main effect as data frame
lm_result <- lm(log_interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
               data=iacc_data,
               na.action=na.omit)


```

Now check the assumptions again =\> got worse.. try another tranformation.. Sqrt-trans: GAVE THE BEST MIDDLE WAY

```{r}
iacc_data$sqrt_interoceptive_accuracy <- sqrt(iacc_data$interoceptive_accuracy + 0.0001)
lm_result <- lm(sqrt_interoceptive_accuracy ~ group + age + sex + education_years + body_mass_index + heart_rate_bpm + smoker + had_caffeine + knows_heartrate + MAIA_not_worrying, 
                data = iacc_data, na.action = na.omit)


```

Now better! Red line looks more str8 and RESET is not significant anymore..

#### TEST IAcc

ANOVA (ignore this bc we have theoretical reasons to do LM =\> see notes on Notion..)

```{r}
# anova() Computes analysis of variance (or deviance) tables for one or more fitted model objects.
anova_table <- anova(lm_result)

# Add predictor and outcome columns
anova_table$predictor <- rownames(anova_table)
rownames(anova_table) <- NULL # now we have a col for predictors, dont need the rownames
anova_table$outcome <- rep("interoceptive_accuracy", times = nrow(anova_table))
# df_den
n <- nobs(lm_result)
k <- length(levels(iacc_data$group))
df_den = n - k
anova_table$df_den <- df_den

# Rename the columns appropriately
colnames(anova_table) = c("df_num","sum_sq","mean_sq","f","p","predictor","outcome", "df_den")

# Reorder the columns
anova_table = anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

# Format p-values to avoid scientific notation
anova_table$p <- format(anova_table$p, scientific = FALSE, digits = 3)
```

Frequentist LM \*\*\*\*\*\*\*\*\*\*\*\*\* !!!!!!! USE THIS ALSO ELSEWHERE

```{r lm}

# 1. Get LM coefficients with CIs (adjusted for covariates)

# Model Summary & Coeffs
summary_lm <- summary(lm_result)
confint_lm <- confint(lm_result)  # 95% CIs for coefficients by default

# Prepare coefficient table
coef_df <- data.frame(
  summary_lm$coefficients,
  confint_lm,
  predictor = rownames(summary_lm$coefficients),
  outcome = "interoceptive_accuracy",
  row.names = NULL
) %>%
  rename(
    estimate = Estimate,
    std_error = Std..Error,
    t_value = t.value,
    p_value = Pr...t..,
    coef_95CI_low = X2.5..,
    coef_95CI_high = X97.5..
  )


# 2. Get standardized (adjusted) betas (effect sizes)
std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate = Std_Coefficient,
    std_95CI_low = CI_low,
    std_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = "interoceptive_accuracy") %>%
  select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)


# 3. Merge raw and standardized results
lm_result_table <- coef_df %>%
  left_join(std_beta_df, by = c("outcome", "predictor")) %>%
  # Reorder columns for clarity
  select(
    predictor, outcome,
    estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
    std_estimate, std_95CI_low, std_95CI_high,           # Standardized
    t_value, p_value
  )

# Filter for your predictor of interest (e.g., "groupSZ")
lm_result_table_SZ <- lm_result_table %>%
  filter(predictor == "groupSSD")  # Replace with your target predictor

# ADD BF10 !!!
# Below
# XX



#SAVE
#write.csv(
#  combined_table_interoceptive_accuracy_eta, 
#  file = file.path(plot_path, "lm_results_interoceptive-accuracy_2025-04-23.csv"),
#  row.names = FALSE
#)

#SAVE
write.csv(combined_table_interoceptive_accuracy_eta, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/conf_full_model_lm_results_interoceptive_accuracy.csv", row.names = FALSE)
write.csv(lm_result_table_SZ, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/conf_SZ_lm_results_interoceptive_accuracy.csv", row.names = FALSE)

### CHECK THIS OUT : useful for the emmeans graph !!!
#emmmip(lm_result)


# get the model fits:  AIC (Akaike Information Criterion), or BIC (Bayesian Information Criterion)
#AIC(lm_result)
#BIC(lm_result)

## ARCHIVE

# Delete? Merge the ANOVA and coefficient tables on the predictor variable
# combined_table_interoceptive_accuracy <- merge(anova_table, coef_df, by = c("outcome", "predictor"), all = TRUE)

# Calculate the effect size (eta-squared)
#effect_size <- etaSquared(lm_result)
#effect_size_df <- as.data.frame(effect_size)
# Add predictor and outcome columns to the coefficients data frame
#effect_size_df$predictor <- rownames(effect_size_df)
#rownames(effect_size_df) <- NULL
#effect_size_df$outcome <- rep("interoceptive_accuracy", times = nrow(effect_size_df))

# Rename coefficient columns for clarity
#colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")

```

Bayesian LM

```{r}

# BAYESIAN LINEAR MIXED MODELS: exercise effects on structural and functional brain patterns  -------------------------------------------------------------------------------------------------------------

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index"),
  set_prior("normal(0, 1)", class = "b", coef = "heart_rate_bpm"),
  set_prior("normal(0, 1)", class = "b", coef = "smoker1"),
  set_prior("normal(0, 1)", class = "b", coef = "had_caffeine1"),
  set_prior("normal(0, 1)", class = "b", coef = "knows_heartrate1"),
  set_prior("normal(0, 1)", class = "b", coef = "MAIA_not_worrying")
)

# Define the model formula
formula <- interoceptive_accuracy ~ group + age + sex + education_years + 
            body_mass_index + heart_rate_bpm + smoker + had_caffeine + 
            knows_heartrate + MAIA_not_worrying + (1 | subject)

# Fit the Bayesian model
bayes_model <- brm(
  formula = formula,
  data = iacc_data,
  prior = priors,
  sample_prior = TRUE,
  chains = 4,
  cores = 4,
  iter = 4000,
  seed = 123
)

# Summary of the Bayesian model
summary(bayes_model)

# Extract posterior estimates
posterior_summary <- posterior_summary(bayes_model)

# Compute model fit statistics like WAIC or LOO
waic_result <- waic(bayes_model)
#loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0",
  "heart_rate_bpm = 0",
  "smoker1 = 0",
  "had_caffeine1 = 0",
  "knows_heartrate1 = 0",
  "MAIA_not_worrying = 0"
)
hypothesis_results <- hypothesis(bayes_model, hypotheses)
df_hypothesis <- data.frame(hypothesis_results$hypothesis)

# Calculate Bayes Factors (BF10)
df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
df_hypothesis <- df_hypothesis %>%
  rename(
    b_estimate = Estimate,
    err = Est.Error,
    CI.low = CI.Lower,
    CI.high = CI.Upper,
    evid.ratio = Evid.Ratio,
    post.prob = Post.Prob,
    sig = Star
  )

write.csv(df_hypothesis, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/IAcc/results_BF_hypotheses_iacc.csv", row.names = FALSE)

 ################# waits for now #########

# Compute pairwise comparisons for group effect
emm <- emmeans(bayes_model, ~group)
contrasts <- contrast(emm, method = "pairwise")
contrast_summary <- summary(contrasts)

# Compute Cohen's d for effect sizes
cohen_d <- function(contrast_estimate, standard_error) {
  return(contrast_estimate / standard_error)
}

contrast_estimates <- contrast_summary$estimate
standard_errors <- contrast_summary$SE
d_values <- mapply(cohen_d, contrast_estimates, standard_errors)

# Combine results into a data frame
effect_sizes_summary <- data.frame(
  Contrast = contrast_summary$contrast,
  Estimate = contrast_estimates,
  SE = standard_errors,
  Cohen_d = d_values,
  CI_low = contrast_summary$lower.CL,
  CI_high = contrast_summary$upper.CL
)

#write.csv(effect_sizes_summary, file = "results_effect_sizes.csv", row.names = FALSE)

# Print results for verification
print(df_hypothesis)
print(effect_sizes_summary)

```

Hierarchical LM to avoid overfitting

```{r}
# First, let's define the order in which we'll add variables
# Start with your primary predictor (group), then add covariates in order of importance
variable_order <- c("group", "sex", "education_years", "MAIA_not_worrying", "heart_rate_bpm", "smoker", "age", "body_mass_index", 
                     "had_caffeine", "knows_heartrate"
                   )

# Initialize a list to store all models
models <- list()

# Create baseline model (intercept only)
models[["intercept_only"]] <- lm(interoceptive_accuracy ~ 1, 
                                data = iacc_data,
                                na.action = na.omit)

# Build models sequentially
current_formula <- "interoceptive_accuracy ~ 1"
for (var in variable_order) {
  current_formula <- paste(current_formula, "+", var)
  models[[current_formula]] <- lm(as.formula(current_formula), 
                                 data = iacc_data,
                                 na.action = na.omit)
}

# Create a model comparison table
model_comparison <- data.frame(
  Model = names(models),
  R_squared = sapply(models, function(x) summary(x)$r.squared),
  Adj_R_squared = sapply(models, function(x) summary(x)$adj.r.squared),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC),
  stringsAsFactors = FALSE
)

# Calculate change in R-squared at each step
model_comparison$R_squared_change <- c(NA, diff(model_comparison$R_squared))
model_comparison$Adj_R_squared_change <- c(NA, diff(model_comparison$Adj_R_squared))

# View the comparison
print(model_comparison)

# save model 
write.csv(model_comparison, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/iacc_model_comparison.csv", row.names = FALSE)

# You can also examine each model's coefficients individually
# For example, to see the final model:
summary(models[[length(models)]])

# winning model:
best_model <- lm(interoceptive_accuracy ~ group + sex, # + education_years + MAIA_not_worrying +heart_rate_bpm,
                data = iacc_data,
                na.action = na.omit)

# Check assumptions
par(mfrow=c(2,2))
plot(best_model) # Residual diagnostics

# Check multicollinearity
car::vif(best_model) # Values >5 indicate problems

# Check influential points
car::influencePlot(best_model)

#  RE RUN BEST MODEL

# Model Summary & Coeffs
summary_lm <- summary(best_model)
confint_lm <- confint(best_model)  # 95% CIs for coefficients by default

# Prepare coefficient table
coef_df <- data.frame(
  summary_lm$coefficients,
  confint_lm,
  predictor = rownames(summary_lm$coefficients),
  outcome = "interoceptive_accuracy",
  row.names = NULL
) %>%
  rename(
    estimate = Estimate,
    std_error = Std..Error,
    t_value = t.value,
    p_value = Pr...t..,
    coef_95CI_low = X2.5..,
    coef_95CI_high = X97.5..
  )


# 2. Get standardized (adjusted) betas (effect sizes)
std_beta_df <- standardize_parameters(best_model, method = "refit") %>%
  as.data.frame() %>%
  rename(
    std_estimate = Std_Coefficient,
    std_95CI_low = CI_low,
    std_95CI_high = CI_high,
    predictor = Parameter
  ) %>%
  mutate(outcome = "interoceptive_accuracy") %>%
  select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)


# 3. Merge raw and standardized results
lm_result_table <- coef_df %>%
  left_join(std_beta_df, by = c("outcome", "predictor")) %>%
  # Reorder columns for clarity
  select(
    predictor, outcome,
    estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
    std_estimate, std_95CI_low, std_95CI_high,           # Standardized
    t_value, p_value
  )

# Filter for your predictor of interest (e.g., "groupSZ")
lm_result_table_SZ <- lm_result_table %>%
  filter(predictor == "groupSSD")  # Replace with your target predictor

# save
write.csv(lm_result_table, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/lm_result_table_best_model_iacc.csv", row.names = FALSE)



```

BF for best model
```{r}

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "heart_rate_bpm"),
  set_prior("normal(0, 1)", class = "b", coef = "MAIA_not_worrying")
)

# Define the model formula
formula <- interoceptive_accuracy ~ group + sex + education_years + 
           heart_rate_bpm + MAIA_not_worrying + (1 | subject)

# Fit the Bayesian model
bayes_model <- brm(
  formula = formula,
  data = iacc_data,
  prior = priors,
  sample_prior = TRUE,
  chains = 4,
  cores = 4,
  iter = 4000,
  seed = 123
)

# Summary of the Bayesian model
summary(bayes_model)

# Extract posterior estimates
posterior_summary <- posterior_summary(bayes_model)

# Compute model fit statistics like WAIC or LOO
waic_result <- waic(bayes_model)
#loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "sexm = 0",
  "education_years = 0",
  "heart_rate_bpm = 0",
  "MAIA_not_worrying = 0"
)
hypothesis_results <- hypothesis(bayes_model, hypotheses)
df_hypothesis <- data.frame(hypothesis_results$hypothesis)

# Calculate Bayes Factors (BF10)
df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
df_hypothesis <- df_hypothesis %>%
  rename(
    b_estimate = Estimate,
    err = Est.Error,
    CI.low = CI.Lower,
    CI.high = CI.Upper,
    evid.ratio = Evid.Ratio,
    post.prob = Post.Prob,
    sig = Star
  )

#write.csv(df_hypothesis, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/IAcc/results_BF_hypotheses_iacc.csv", row.names = FALSE)

```


#### PLOT IAcc

```{r}

plot_path = '/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/'

# Create a violin plot of BPQ_total by group
iacc_plot <- ggplot(iacc_data, aes(x = group, y = interoceptive_accuracy, fill = group)) +
  geom_boxplot(trim = FALSE) +  # Show the full distribution
  geom_jitter(width = 0.2, alpha = 0.5) +  # Show individual data points
  labs(title = "Interoceptive Accuracy by Group", 
       x = "Group", 
       y = "Interoceptive Accuracy") +
  theme_minimal() +
  theme ( legend.position = c(0.9, 0.8),
          legend.text = element_text(size = 18),  # Increase legend text size
          legend.title = element_text(size = 18), # Increase legend title size
          legend.key.size = unit(1.5, "cm"), 
          plot.title = element_text(size = 18, face = "bold"),  # Title size and bold
          axis.title = element_text(size = 18),   # Axis title size
          axis.text = element_text(size = 14),    # Axis text size
          strip.text = element_text(size = 16)  ) +
  scale_fill_brewer(palette = "Set3")  # Optional: better color scheme

iacc_plot

# Save the plot with an appropriate name and path
ggsave(filename = paste0(plot_path, "interoceptive-accuracy_boxplot_by_group_raw.png"),         plot = iacc_plot,
        width = 8, height = 6, dpi = 300)

#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/Interoceptive Accuracy/MBB_results/interoceptive_accuracy_boxplot_2025-02-21.jpg", plot = iacc_plot, width = 12, height = 12, dpi = 300)


```

plot adjusted? not sure if correct

```{r}
# Get adjusted values (residuals + group effect)
adj_data <- iacc_data %>%
  mutate(
    # Remove covariate effects (add fixed intercept)
    adjusted = predict(model, re.form = NA) + residuals(model)
  )

ggplot(adj_data, aes(x = group, y = adjusted, fill = group)) +
  geom_violin(trim = FALSE, alpha = 0.7) +
  geom_boxplot(width = 0.2, alpha = 0.8) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  
  labs(
    title = "Interoceptive Accuracy (Adjusted for Covariates)",
    x = "Group",
    y = "Adjusted Interoceptive Accuracy",
    caption = "Values adjusted for age, sex, BMI, and other covariates"
  ) +
  
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  ) +
  
  scale_fill_brewer(palette = "Set2")

```

### 1.3) At rest, the patients’ Heartbeat Evoked Potential amplitude (HEP; 450-500 ms post-ECG R-peak, in frontal central regions, specifically for the Fp2, F4, F8 electrodes) is different than HC, denoting altered interoception on the neural level. (non-directional hypothesis).

We will run 2×3 Analyses of Covariance (ANCOVA) to inspect the HEP differences between the groups (between-subject variable with 2 levels: HC and SSD) across different tasks (within-subject variable with 3 levels: eyes-closed, eyes-open, HCT), while controlling for the covariates: age, gender, body mass index (BMI), educational level, smoking, caffeine intake, HR, QT interval, QTc interval, the R wave amplitude, and Root Mean Square of Successive Differences (RMSSD), and the Not-Worrying subscale of MAIA as a proxy for anxiety about bodily sensations.

Utilizing an ANCOVA will allow us to scrutinize the differential HEP modulation through directed attention between groups (significant interaction effects will be tested with Tukey’s HSD post-hoc tests). Parallel to Koreki and colleagues (2024), we will focus on the frontal-central regions. We will run an ANCOVA on the HEP amplitude averaged across all frontal-central areas, as well as running individual ANCOVAs for each electrode within this region ('Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 'FC1', 'FC2', 'FC5', 'FC6', 'C3', 'C4', 'Cz'). To address the issue of multiple comparisons and identify assembles of significant electrodes, we will conduct a cluster-based correction analysis.

convert amplitudes from volts to microVolts (not needed for the mean_amplitude col...)

```{r}
# Convert amplitudes from volts to microvolts
roi_hep_data$F4_mean_amplitude <- roi_hep_data$F4_mean_amplitude * 1e6
roi_hep_data$F8_mean_amplitude <- roi_hep_data$F8_mean_amplitude * 1e6
roi_hep_data$Fp2_mean_amplitude <- roi_hep_data$Fp2_mean_amplitude * 1e6


```

Define Task order

```{r}
# Reorder the 'task' variable to appear in the desired order
task_order <- c("eyes-closed", "eyes-open", "hct")
roi_hep_data$task <- factor(roi_hep_data$task, levels = task_order)

```

get channel names

```{r channels, frontal_central_regions}
# I can get any since they are all the same
channels <- roi_hep_data$channels[1]

# Define the character vector
frontal_central_regions <- c('Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz', 
                              'FC1', 'FC2', 'FC5', 'FC6', 'C3', 'C4', 'Cz')

hypothesis_regions <- c("Fp2", "F4", "F8")
hypothesis_vars <- c("Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude")


```

Basic analysis without covars:
```{r}
# Define dependent variables
dependent_vars <- c("Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude")

# Initialize a list to store results for both datasets
lm_results <- list()

# Loop through dependent variables
for (dv in dependent_vars) {
  # Model 1: With task (roi_hep_data)
  formula_with_task <- as.formula(paste(dv, "~ group * task"))
  lm_with_task <- lm(formula_with_task, data = roi_hep_data)
  summary_with_task <- summary(lm_with_task)
  
  # Store results for the model with task
  lm_results[[dv]]$with_task <- list(
    formula = formula_with_task,
    summary = summary_with_task,
    coefficients = summary_with_task$coefficients
  )
  
  # Print coefficients for the model with task
  print(paste("Results for", dv, "with task:"))
  print(summary_with_task$coefficients)
  
  # Model 2: Without task (roi_hep_data_ec)
  formula_without_task <- as.formula(paste(dv, "~ group"))
  lm_without_task <- lm(formula_without_task, data = roi_hep_data_ec)
  summary_without_task <- summary(lm_without_task)
  
  # Store results for the model without task
  lm_results[[dv]]$without_task <- list(
    formula = formula_without_task,
    summary = summary_without_task,
    coefficients = summary_without_task$coefficients
  )
  
  # Print coefficients for the model without task
  print(paste("Results for", dv, "without task:"))
  print(summary_without_task$coefficients)
}

# Access stored results for a specific variable (e.g., Fp2_mean_amplitude with task)
print(lm_results[["Fp2_mean_amplitude"]]$with_task$coefficients)

# Access stored results for the same variable without task
print(lm_results[["Fp2_mean_amplitude"]]$without_task$coefficients)

```

1)  Three hypothesis-driven Channels

WORKS!NEW LM ONLY: Only confirmatory analysis with eyes-closed THEN exploratory with task!

```{r}
roi_hep_data_ec <- roi_hep_data %>%
  filter(task == "eyes-closed")


# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs/"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Do PCA on correlated vars
  correlated_vars <- roi_hep_data_ec[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
  correlated_vars_scaled <- scale(correlated_vars)
  pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
  summary(pca_result)
  plot(pca_result, type = "l")  # Scree plot
  roi_hep_data_ec$PC1 <- pca_result$x[, 1]  # First principal component
  roi_hep_data_ec$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index  + smoker + had_caffeine +
                       PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = roi_hep_data_ec,
                  na.action = na.omit)

  
  # a. Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # 6. Outliers
  plot(lm_result, which = 4)
  # Outlier Test
  outlierTest(lm_result)
  
  # 7. Additional Diagnostics
  resettest_result <- resettest(lm_result)
  print(paste( "Resettest Result", var, ":", resettest_result))
  print(resettest_result)

  
  # b. Get LM coefficients with CIs (adjusted for covariates)

  # Model Summary & Coeffs
  summary_lm <- summary(lm_result)
  confint_lm <- confint(lm_result)  # 95% CIs for coefficients by default
  
  # Prepare coefficient table
  coef_df <- data.frame(
    summary_lm$coefficients,
    confint_lm,
    predictor = rownames(summary_lm$coefficients),
    outcome = var,
    row.names = NULL
  ) %>%
    rename(
      estimate = Estimate,
      std_error = Std..Error,
      t_value = t.value,
      p_value = Pr...t..,
      coef_95CI_low = X2.5..,
      coef_95CI_high = X97.5..
    )

    
  # c. Get standardized (adjusted) betas (effect sizes)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
    as.data.frame() %>%
    rename(
      std_estimate = Std_Coefficient,
      std_95CI_low = CI_low,
      std_95CI_high = CI_high,
      predictor = Parameter
    ) %>%
    mutate(outcome = var) %>%
    select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)
  
  
  # d. Merge raw and standardized results
  lm_result_table <- coef_df %>%
    left_join(std_beta_df, by = c("outcome", "predictor")) %>%
    # Reorder columns for clarity
    select(
      predictor, outcome,
      estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
      std_estimate, std_95CI_low, std_95CI_high,           # Standardized
      t_value, p_value
    )
  
  # Filter for your predictor of interest (e.g., "groupSZ")
  lm_result_table_SZ <- lm_result_table %>%
    filter(predictor == "groupSSD")  # Replace with your target predictor
  
  # Store the combined results in the list
  combined_results[[var]] <- lm_result_table
  
}

# Combine all results into one dataframe
lm_results_df_heps <- do.call(rbind, combined_results)
# save 
write.csv(lm_results_df_heps, file = file.path(plot_path, "lm_results_hep_ec_2025-05-05.csv"), row.names = FALSE)


# Filter results for group comparison
lm_results_df_heps_groupSZ <- lm_results_df_heps[
  lm_results_df_heps$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
lm_results_df_heps_groupSZ$p_adjusted <- p.adjust(lm_results_df_heps_groupSZ$p_value, method = "BH") 

# View the results
print(lm_results_df_heps_groupSZ)

#save
write.csv(lm_results_df_heps_groupSZ, file = file.path(plot_path, "lm_results_hep_ec_lm-groupSZ_p-adjusted_2025-05-05.csv"), row.names = FALSE)

```

BF...

```{r}
# BAYESIAN LINEAR MIXED MODELS: exercise effects on structural and functional brain patterns  -------------------------------------------------------------------------------------------------------------

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index"),
  set_prior("normal(0, 1)", class = "b", coef = "smoker1"),
  set_prior("normal(0, 1)", class = "b", coef = "had_caffeine1"),
  set_prior("normal(0, 1)", class = "b", coef = "PC1"),
  set_prior("normal(0, 1)", class = "b", coef = "PC2"),
  set_prior("normal(0, 1)", class = "b", coef = "hrv_rmssd_ms"),
  set_prior("normal(0, 1)", class = "b", coef = "R_peak_amplitude_mV"),
  set_prior("normal(0, 1)", class = "b", coef = "MAIA_not_worrying")
)

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0",
  "smoker1 = 0",
  "had_caffeine1 = 0",
  "PC1 = 0",
  "PC2 = 0",
  "hrv_rmssd_ms = 0",
  "R_peak_amplitude_mV = 0",
  "MAIA_not_worrying = 0"
)

# initialize empty list
combined_results_bf <- list()

#Loop
for (var in hypothesis_vars) { # or questionnaire_vars

  # Define the model formula
  formula <- as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index  + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying + (1 | subject)"))
  
  # Fit the Bayesian model
  bayes_model <- brm(formula = formula, data = roi_hep_data_ec, prior = priors, sample_prior = TRUE, chains = 4, cores = 4, iter = 4000, seed = 123)
  
  # Summary of the Bayesian model
  summary(bayes_model)
  
  # Extract posterior estimates
  posterior_summary <- posterior_summary(bayes_model)
  
  # Compute model fit statistics like WAIC or LOO
  waic_result <- waic(bayes_model)
  #loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?
  
  ## H1 results
  hypothesis_results <- hypothesis(bayes_model, hypotheses)
  df_hypothesis <- data.frame(hypothesis_results$hypothesis)
  
  # Calculate Bayes Factors (BF10)
  df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
  df_hypothesis <- df_hypothesis %>%
    rename(
      b_estimate = Estimate,
      err = Est.Error,
      CI.low = CI.Lower,
      CI.high = CI.Upper,
      evid.ratio = Evid.Ratio,
      post.prob = Post.Prob,
      sig = Star
    ) %>%
    mutate(outcome = var) 
  
  # Store the combined results in the list
  combined_results_bf[[var]] <- df_hypothesis
}

# Combine all results into one dataframe
bf_lm_results_df_heps <- do.call(rbind, combined_results_bf)
# save 
write.csv(bf_lm_results_df_heps, file = file.path(plot_path, "bf_lm_results_hep_ec_2025-05-06.csv"), row.names = FALSE)

```

WORKS! OLD (with ANOVA): Only confirmatory analysis with eyes-closed THEN exploratory with task!

```{r}
roi_hep_data_ec <- roi_hep_data %>%
  filter(task == "eyes-closed")


# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs/"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Do PCA on correlated vars
  correlated_vars <- roi_hep_data_ec[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
  correlated_vars_scaled <- scale(correlated_vars)
  pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
  summary(pca_result)
  plot(pca_result, type = "l")  # Scree plot
  roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
  roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine +
                       PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = roi_hep_data,
                  na.action = na.omit)

  
  # a. Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # 6. Outliers
  plot(lm_result, which = 4)
  # Outlier Test
  outlierTest(lm_result)
  
  # 7. Additional Diagnostics
  resettest(lm_result)

  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(lm_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  #combined_table <- full_join(anova_table, coef_df, effect_size_df, by = c("outcome", "predictor"))
  combined_table <- full_join(anova_table, coef_df, by = c("outcome", "predictor")) %>%
  full_join(effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
}

# Combine all results into one dataframe
anova_results_df_heps <- do.call(rbind, combined_results)
# save 
# write.csv(anova_results_df_heps, file = file.path(plot_path, "lm_results_hep_ec_2025-04-24.csv"), row.names = FALSE)


# Filter results for group comparison
anova_results_df_heps_groupSZ <- anova_results_df_heps[
  anova_results_df_heps$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_heps_groupSZ$p_adjusted <- p.adjust(anova_results_df_heps_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_heps_groupSZ)

#save
# write.csv(anova_results_df_heps_groupSZ, file = file.path(plot_path, "lm_results_hep_ec_lm-groupSZ_p-adjusted_2025-04-24.csv"), row.names = FALSE)


### FOR ANCOVA 
# Filter results for group comparison
anova_results_df_heps_group <- anova_results_df_heps[
  anova_results_df_heps$predictor %in% c("group"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_heps_group$p_adjusted <- p.adjust(anova_results_df_heps_group$p, method = "BH") 

# View the results
print(anova_results_df_heps_group)

# write.csv(anova_results_df_heps_group, file = file.path(plot_path, "lm_results_hep_ec_anova-group_p-adjusted_2025-04-24.csv"), row.names = FALSE)



# SAVE !!!
# Save the data frame as a CSV file to the directory
# write.csv(anova_results_df_questionnaires_total_groupSZ, file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-04-23.csv"), row.names = FALSE)

```

WORKS! NEW LM ONLY: Exploratory all 3 including task!

```{r}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs/"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Do PCA on correlated vars
  correlated_vars <- roi_hep_data[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
  correlated_vars_scaled <- scale(correlated_vars)
  pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
  summary(pca_result)
  plot(pca_result, type = "l")  # Scree plot
  roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
  roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine +
                       PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = roi_hep_data,
                  na.action = na.omit)
  

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
    
  # 6. Outliers
  plot(lm_result, which = 4)
  # Outlier Test
  outlierTest(lm_result)
  
  # 7. Additional Diagnostics
  resettest_result <- resettest(lm_result)
  print(paste( "Resettest Result", var, ":", resettest_result))
  print(resettest_result)

   
  # b. Get LM coefficients with CIs (adjusted for covariates)
  # Model Summary & Coeffs
  summary_lm <- summary(lm_result)
  confint_lm <- confint(lm_result)  # 95% CIs for coefficients by default
  
  # Prepare coefficient table
  coef_df <- data.frame(
    summary_lm$coefficients,
    confint_lm,
    predictor = rownames(summary_lm$coefficients),
    outcome = var,
    row.names = NULL
  ) %>%
    rename(
      estimate = Estimate,
      std_error = Std..Error,
      t_value = t.value,
      p_value = Pr...t..,
      coef_95CI_low = X2.5..,
      coef_95CI_high = X97.5..
    )

    
  # c. Get standardized (adjusted) betas (effect sizes)
  std_beta_df <- standardize_parameters(lm_result, method = "refit") %>%
    as.data.frame() %>%
    rename(
      std_estimate = Std_Coefficient,
      std_95CI_low = CI_low,
      std_95CI_high = CI_high,
      predictor = Parameter
    ) %>%
    mutate(outcome = var) %>%
    select(predictor, outcome, std_estimate, std_95CI_low, std_95CI_high)
  
  
  # d. Merge raw and standardized results
  lm_result_table <- coef_df %>%
    left_join(std_beta_df, by = c("outcome", "predictor")) %>%
    # Reorder columns for clarity
    select(
      predictor, outcome,
      estimate, std_error, coef_95CI_low, coef_95CI_high,  # Raw estimates
      std_estimate, std_95CI_low, std_95CI_high,           # Standardized
      t_value, p_value
    )
  
  # Filter for your predictor of interest (e.g., "groupSZ")
  lm_result_table_SZ <- lm_result_table %>%
    filter(predictor == "groupSSD")  # Replace with your target predictor
  
  # Store the combined results in the list
  combined_results[[var]] <- lm_result_table
  
  ### PLOT
  emm <- emmeans(lm_result, ~ group * task)
  pairwise_results <- pairs(emm)
  summary(emm)
  plot(emm)

  # Convert emmeans results to a data frame
  emm_df <- as.data.frame(emm)

  # Plot using ggplot2 with larger axis labels and a title
    emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
    geom_line(size = 1.5) +          # Line for each group
    geom_point(size = 4) + # Points for each group
    geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
    scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
    scale_y_continuous(limits = c(-1, 1)) +  # Fixed y-axis scale (adjust limits as needed)
    labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
    ggtitle(paste(var, "HEP Mean Amplitude"))+  # Add the title
    theme_minimal() +      # Minimal theme for clean look
    theme(
      axis.title = element_text(size = 20),    # Increase axis title size
      axis.text = element_text(size = 20),     # Increase axis text size
      legend.text = element_text(size = 14),   # Increase legend text size
      legend.title = element_text(size = 16),  # Increase legend title size
      strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
      plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

  print(emmeans_plot)
  
  # SAVE
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Emmeans_Group_Task", var, "_2025-05-05.png"), plot = emmeans_plot, width = 8, height = 6, dpi = 300)
}

# Combine all results into one dataframe
lm_results_df_heps <- do.call(rbind, combined_results)
# save 
write.csv(lm_results_df_heps, file = file.path(plot_path, "lm_results_hep_full-model-rois_2025-05-05.csv"), row.names = FALSE)


### ADJUST FOR MULTI COMPS due 3 ROI.. (ANOVA)

# Define the terms of interest
terms_of_interest <- c("groupSSD", "taskeyes-open", "taskhct","groupSSD:taskeyes-open", "groupSSD:taskhct")

# Initialize a list to store the results
adjusted_results <- list()

# Apply the filtering and correction for each term
for (term in terms_of_interest) {
  # Filter results for the current term
  results_filtered <- lm_results_df_heps[
    lm_results_df_heps$predictor == term, 
  ]
  
  # Correct for multiple comparisons using the Benjamini-Hochberg procedure
  results_filtered$p_adjusted <- p.adjust(results_filtered$p_value, method = "BH")
  
  # Save to the list
  adjusted_results[[term]] <- results_filtered
}

# Combine all adjusted results into one dataframe
combined_adjusted_results <- do.call(rbind, adjusted_results)

# Write the combined results to a single CSV file
output_filename <- "lm_results_hep_full-model_rois_anova_combined_p-adjusted_2025-05-05.csv"
write.csv(combined_adjusted_results, file = file.path(plot_path, output_filename), row.names = FALSE)

# View the combined results
print(combined_adjusted_results)


```

BF

```{r}
# BAYESIAN LINEAR MIXED MODELS: exercise effects on structural and functional brain patterns  -------------------------------------------------------------------------------------------------------------

# Define priors for Bayesian model
priors <- c(
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD"),
  set_prior("normal(0, 1)", class = "b", coef = "taskeyesMopen"),
  set_prior("normal(0, 1)", class = "b", coef = "taskhct"),
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD:taskeyesMopen"),
  set_prior("normal(0, 1)", class = "b", coef = "groupSSD:taskhct"),
  set_prior("normal(0, 1)", class = "b", coef = "age"),
  set_prior("normal(0, 1)", class = "b", coef = "sexm"),
  set_prior("normal(0, 1)", class = "b", coef = "education_years"),
  set_prior("normal(0, 1)", class = "b", coef = "body_mass_index"),
  set_prior("normal(0, 1)", class = "b", coef = "smoker1"),
  set_prior("normal(0, 1)", class = "b", coef = "had_caffeine1"),
  set_prior("normal(0, 1)", class = "b", coef = "PC1"),
  set_prior("normal(0, 1)", class = "b", coef = "PC2"),
  set_prior("normal(0, 1)", class = "b", coef = "hrv_rmssd_ms"),
  set_prior("normal(0, 1)", class = "b", coef = "R_peak_amplitude_mV"),
  set_prior("normal(0, 1)", class = "b", coef = "MAIA_not_worrying")
)

# Hypothesis Testing
hypotheses <- c(
  "groupSSD = 0",
  "taskeyesMopen = 0",
  "taskhct = 0",
  "groupSSD:taskeyesMopen = 0",
  "groupSSD:taskhct = 0",
  "age = 0",
  "sexm = 0",
  "education_years = 0",
  "body_mass_index = 0",
  "smoker1 = 0",
  "had_caffeine1 = 0",
  "PC1 = 0",
  "PC2 = 0",
  "hrv_rmssd_ms = 0",
  "R_peak_amplitude_mV = 0",
  "MAIA_not_worrying = 0"
)

# initialize empty list
combined_results_bf <- list()

#Loop
for (var in hypothesis_vars) { # or questionnaire_vars

  # Define the model formula
  formula <- as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying + (1 | subject)"))
  
  # default priors
  #default_priors <- default_prior(formula = formula, data = roi_hep_data)
  
  # Fit the Bayesian model
  bayes_model <- brm(formula = formula, data = roi_hep_data, prior = priors, sample_prior = TRUE, chains = 4, cores = 4, iter = 4000, seed = 123)
  
  # Summary of the Bayesian model
  summary(bayes_model)
  
  # Extract posterior estimates
  posterior_summary <- posterior_summary(bayes_model)
  
  # Compute model fit statistics like WAIC or LOO
  waic_result <- waic(bayes_model)
  #loo_result <- loo(bayes_model, moment_match = TRUE) CHECK THIS?
  
  ## H1 results
  hypothesis_results <- hypothesis(bayes_model, hypotheses)
  df_hypothesis <- data.frame(hypothesis_results$hypothesis)
  
  # Calculate Bayes Factors (BF10)
  df_hypothesis$BF10 <- 1 / abs(df_hypothesis$Evid.Ratio)
  df_hypothesis <- df_hypothesis %>%
    rename(
      b_estimate = Estimate,
      err = Est.Error,
      CI.low = CI.Lower,
      CI.high = CI.Upper,
      evid.ratio = Evid.Ratio,
      post.prob = Post.Prob,
      sig = Star
    ) %>%
    mutate(outcome = var) 
  
  # Store the combined results in the list
  combined_results_bf[[var]] <- df_hypothesis
}

# Combine all results into one dataframe
bf_lm_results_df_heps_interactions <- do.call(rbind, combined_results_bf)
# save 
write.csv(bf_lm_results_df_heps_interactions, file = file.path(plot_path, "bf_lm_results_hep_interactions-full_2025-05-06.csv"), row.names = FALSE)
```

WORKS! (OLD with ANOVA): Exploratory all 3 including task!

```{r}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs/"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Do PCA on correlated vars
  correlated_vars <- roi_hep_data[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
  correlated_vars_scaled <- scale(correlated_vars)
  pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
  summary(pca_result)
  plot(pca_result, type = "l")  # Scree plot
  roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
  roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine +
                       PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = roi_hep_data,
                  na.action = na.omit)
  

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(lm_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  #combined_table <- full_join(anova_table, coef_df, effect_size_df, by = c("outcome", "predictor"))
  combined_table <- full_join(anova_table, coef_df, by = c("outcome", "predictor")) %>%
  full_join(effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
  
  ### PLOT
  emm <- emmeans(lm_result, ~ group * task)
  pairwise_results <- pairs(emm)
  summary(emm)
  plot(emm)

  # Convert emmeans results to a data frame
  emm_df <- as.data.frame(emm)

  # Plot using ggplot2 with larger axis labels and a title
    emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
    geom_line(size = 1.5) +          # Line for each group
    geom_point(size = 4) + # Points for each group
    geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
    scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
    scale_y_continuous(limits = c(-1, 1)) +  # Fixed y-axis scale (adjust limits as needed)
    labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
    ggtitle(paste(var, "HEP Mean Amplitude"))+  # Add the title
    theme_minimal() +      # Minimal theme for clean look
    theme(
      axis.title = element_text(size = 20),    # Increase axis title size
      axis.text = element_text(size = 20),     # Increase axis text size
      legend.text = element_text(size = 14),   # Increase legend text size
      legend.title = element_text(size = 16),  # Increase legend title size
      strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
      plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

  print(emmeans_plot)
  
  # SAVE
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Emmeans_Group_Task", var, ".png"),
         plot = emmeans_plot,
         width = 8, height = 6, dpi = 300)
}

# Combine all results into one dataframe
anova_results_df_heps <- do.call(rbind, combined_results)
# save 
write.csv(anova_results_df_heps, file = file.path(plot_path, "lm_results_hep_full-model-rois_2025-04-24.csv"), row.names = FALSE)


### ADJUST FOR MULTI COMPS due 3 ROI.. (ANOVA)

# Define the terms of interest
terms_of_interest <- c("group", "task", "group:task")

# Initialize a list to store the results
adjusted_results <- list()

# Apply the filtering and correction for each term
for (term in terms_of_interest) {
  # Filter results for the current term
  results_filtered <- anova_results_df_heps[
    anova_results_df_heps$predictor == term, 
  ]
  
  # Correct for multiple comparisons using the Benjamini-Hochberg procedure
  results_filtered$p_adjusted <- p.adjust(results_filtered$p, method = "BH")
  
  # Save to the list
  adjusted_results[[term]] <- results_filtered
}

# Combine all adjusted results into one dataframe
combined_adjusted_results <- do.call(rbind, adjusted_results)

# Write the combined results to a single CSV file
output_filename <- "lm_results_hep_full-model_rois_anova_combined_p-adjusted_2025-04-24.csv"
write.csv(
  combined_adjusted_results, 
  file = file.path(plot_path, output_filename), 
  row.names = FALSE
)

# View the combined results
print(combined_adjusted_results)

### IF group:task is significant; do post-hoc comparisons, correcting for the multiple comparisons!!


# SAVE !!!
# Save the data frame as a CSV file to the directory
# write.csv(anova_results_df_questionnaires_total_groupSZ, file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-04-23.csv"), row.names = FALSE)

```

Exploratory all 3 including task!

```{r test all three with lm & check assumptions in a loop}

# where the plots should be saved
plot_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs/"

# Initialize a list to store the results
combined_results <- list()

for (var in hypothesis_vars) { # or questionnaire_vars
  
  # Fit the linear model
  lm_result <- lm(as.formula(paste(var, "~ group + age + sex + education_years + body_mass_index")), 
                  data = roi_hep_data,
                  na.action = na.omit)
  
  # Identify rows used in the linear model
  used_rows <- !is.na(questionnaire_data$group) & 
                complete.cases(questionnaire_data[, c(var, "group", "age", "sex", "education_years", "body_mass_index")])
  
  # Create a new dataframe with residuals and corresponding groups
  residuals_data <- data.frame(
    group = questionnaire_data$group[used_rows],
    residuals = residuals(lm_result)
  )
  
  # Step 2: Create a boxplot of residuals by group
  p <- ggplot(residuals_data, aes(x = group, y = residuals, fill = group)) +
    geom_boxplot(outlier.shape = NA, alpha = 0.7) +  # Boxplot without separate outlier points
    geom_jitter(width = 0.2, alpha = 0.5, color = "black") +  # Show individual data points
    labs(title = paste("Distribution of Residuals by Group for", var), 
         x = "Group", 
         y = "Residuals") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set3")  # Optional: better color scheme
  
  print(p)
  
  # Save the plot with an appropriate name and path
  ggsave(filename = paste0(plot_path, "Residuals_by_Group_", var, ".png"),
         plot = p,
         width = 8, height = 6, dpi = 300)

  
  # Check assumptions for the model
  print(paste("Assumption checks for:", var))
  
  # 1. Linearity - Residuals vs Fitted
  plot(lm_result, which = 1, main = paste("Residuals vs Fitted for", var))
  
  # 2. Homoscedasticity - Scale-Location Plot
  plot(lm_result, which = 3, main = paste("Scale-Location Plot for", var))
  
  # 3. Normality of Residuals - Q-Q Plot
  plot(lm_result, which = 2, main = paste("Normal Q-Q Plot for", var))
  
  # 4. Independence of Residuals - Durbin-Watson Test
  dw_test <- dwtest(lm_result)
  print(paste("Durbin-Watson Test for Independence for", var, ": p =", dw_test$p.value))
  
  # 5. Multicollinearity - Variance Inflation Factor (VIF)
  vif_values <- vif(lm_result)
  print(paste("VIF for predictors in", var, ":"))
  print(vif_values)
  
  # Get ANOVA table
  anova_table <- anova(lm_result)
  
  # Add predictor and outcome columns
  anova_table$predictor <- rownames(anova_table)
  rownames(anova_table) <- NULL
  anova_table$outcome <- rep(var, times = nrow(anova_table))
  
  # Calculate df_den
  n <- nobs(lm_result)
  k <- length(levels(questionnaire_data$group))
  df_den <- n - k
  anova_table$df_den <- df_den
  
  # Rename columns
  colnames(anova_table) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  anova_table <- anova_table[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]
  
   # Extract coefficients from the linear model
  summary_lm <- summary(lm_result)
  coefficients <- summary_lm$coefficients
  
  # Convert coefficients to a data frame
  coef_df <- as.data.frame(coefficients)
  
  # Add predictor and outcome columns to the coefficients data frame
  coef_df$predictor <- rownames(coef_df)
  rownames(coef_df) <- NULL
  coef_df$outcome <- rep(var, times = nrow(coef_df))
  
  # Rename coefficient columns for clarity
  colnames(coef_df) <- c("estimate", "std_error", "t_value", "p_value", "predictor", "outcome")
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  combined_table <- merge(anova_table, coef_df, by = c("outcome", "predictor"), all = TRUE)
  
  # Store the combined results in the list
  combined_results[[var]] <- combined_table
}

# Combine all results into one dataframe
anova_results_df_questionnaires_total <- do.call(rbind, combined_results)

# Filter results for group comparison
anova_results_df_questionnaires_total_groupSZ <- anova_results_df_questionnaires_total[
  anova_results_df_questionnaires_total$predictor %in% c("groupSSD"), 
]

# Correct for multiple comparisons for 3 scales using the Benjamini-Hochberg procedure
anova_results_df_questionnaires_total_groupSZ$p_adjusted <- p.adjust(anova_results_df_questionnaires_total_groupSZ$p_value, method = "BH") 

# View the results
print(anova_results_df_questionnaires_total_groupSZ)

# SAVE !!!
# Save the data frame as a CSV file to the directory
write.csv(
  anova_results_df_questionnaires_total_groupSZ, 
  file = file.path(plot_path, "lm_results_questionnaires_group_p-adjusted_2025-04-23.csv"),
  row.names = FALSE
)
```

Hierarchical LM to avoid overfitting

```{r}

# Set up paths and variables
output_dir <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/HEPs"

variable_order <- c("group","body_mass_index", "R_peak_amplitude_mV",  "hrv_rmssd_ms", "heart_rate_bpm", "QTc_interval_ms",
                    "QT_interval_ms", 
                     "sex", "age", "education_years", "MAIA_not_worrying", 
                     "smoker", "had_caffeine",
                    "PC2", "PC1"
                    )

hypothesis_vars <- c("Fp2_mean_amplitude", "F4_mean_amplitude", "F8_mean_amplitude")

#covariates <- c("age", "sex", "education_years", "body_mass_index", "smoker",  "had_caffeine", "PC1", "PC2", "hrv_rmssd_ms", "R_peak_amplitude_mV")


# Initialize a list to store all models
models <- list()

# Create baseline model (intercept only)
models[["intercept_only"]] <- lm(interoceptive_accuracy ~ 1, 
                                data = iacc_data,
                                na.action = na.omit)

# Build models sequentially
current_formula <- "Fp2_mean_amplitude ~ 1"
for (var in variable_order) {
  current_formula <- paste(current_formula, "+", var)
  models[[current_formula]] <- lm(as.formula(current_formula), 
                                 data = roi_hep_data_ec,
                                 na.action = na.omit)
}

# Create a model comparison table
model_comparison <- data.frame(
  Model = names(models),
  R_squared = sapply(models, function(x) summary(x)$r.squared),
  Adj_R_squared = sapply(models, function(x) summary(x)$adj.r.squared),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC),
  stringsAsFactors = FALSE
)

# Calculate change in R-squared at each step
model_comparison$R_squared_change <- c(NA, diff(model_comparison$R_squared))
model_comparison$Adj_R_squared_change <- c(NA, diff(model_comparison$Adj_R_squared))

# View the comparison
print(model_comparison)

# save model 
#write.csv(model_comparison, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/IAcc/model_comparison_iacc_2025-05-21.csv", row.names = FALSE)

# You can also examine each model's coefficients individually
# For example, to see the final model:
summary(models[[length(models)]])

# winning model:
best_model <- lm(interoceptive_accuracy ~ group + sex + education_years + MAIA_not_worrying +heart_rate_bpm,
                data = iacc_data,
                na.action = na.omit)

# Check assumptions
par(mfrow=c(2,2))
plot(best_model) # Residual diagnostics

# Check multicollinearity
car::vif(best_model) # Values >5 indicate problems

# Check influential points
car::influencePlot(best_model)



```

Hierarchical Loop

```{r}
# Initialize a list to store models for each DV
all_models <- list()

# Loop through each DV in hypothesis_vars
for (dv in hypothesis_vars) {
  # Initialize a list to store models for the current DV
  models <- list()
  
  # Start with the baseline model (intercept only)
  current_formula <- paste(dv, "~ 1")
  models[["intercept_only"]] <- lm(as.formula(current_formula), 
                                   data = roi_hep_data_ec, 
                                   na.action = na.omit)
  
  # Incrementally add IVs from variable_order
  for (iv in variable_order) {
    current_formula <- paste(current_formula, "+", iv)
    models[[current_formula]] <- lm(as.formula(current_formula), 
                                    data = roi_hep_data_ec, 
                                    na.action = na.omit)
  }
  
  # Store models for the current DV
  all_models[[dv]] <- models
}

# Prepare the model comparison table
library(dplyr)
model_comparison <- do.call(rbind, lapply(names(all_models), function(dv) {
  models <- all_models[[dv]]
  data.frame(
    Dependent_Variable = dv,
    Model = names(models),
    R_squared = sapply(models, function(x) summary(x)$r.squared),
    Adj_R_squared = sapply(models, function(x) summary(x)$adj.r.squared),
    AIC = sapply(models, AIC),
    BIC = sapply(models, BIC),
    stringsAsFactors = FALSE
  )
}))

# Add change in R-squared for each DV
model_comparison <- model_comparison %>%
  group_by(Dependent_Variable) %>%
  mutate(
    R_squared_change = c(NA, diff(R_squared)),
    Adj_R_squared_change = c(NA, diff(Adj_R_squared))
  ) %>%
  ungroup()

# View the comparison
print(model_comparison)

```

ARCHIVE: Check Q-Q plots

```{r}
roi_hep_data
ancova_result

# Extract residuals from your model
residuals <- residuals(ancova_result)

residuals <- residuals(lm_result)

residuals <- residuals(lm_result_pca)

# QQ plot
qqnorm(residuals, main = "QQ Plot of Residuals")
qqline(residuals, col = "blue", lwd = 2)  # Add a reference line

# QQPLOT LOOKS BETTER FOR LM USE THAT? have too lil data and too many covars !!!
# check univariate models for each covariate and look to find most important !!


```

NOTE: Nothing was significant now, so no need, but in the future, you may need to correct also these 3 tests for multiple comparisons! 

F4: CHECK NORMALITY ASSUMPTION

```{r F4 ancova / lm / emmeans}

# Perform ANCOVA
ancova_result <- aov(F4_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

# Perform LM
lm_result <- lm(F4_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)


# Summary of the ANCOVA result
summary(ancova_result)

emm <- emmeans(ancova_result, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)

# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Plot using ggplot2 with larger axis labels and a title
emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
  geom_line(size = 1.5) +          # Line for each group
  geom_point(size = 4) + # Points for each group
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
  scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
  scale_y_continuous(limits = c(-1, 1)) +  # Fixed y-axis scale (adjust limits as needed)
  labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
  ggtitle("F4 HEP Mean Amplitude") +  # Add the title
  theme_minimal() +      # Minimal theme for clean look
  theme(
    axis.title = element_text(size = 20),    # Increase axis title size
    axis.text = element_text(size = 20),     # Increase axis text size
    legend.text = element_text(size = 14),   # Increase legend text size
    legend.title = element_text(size = 16),  # Increase legend title size
    strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
    plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

print(emmeans_plot)

# SAVE
# ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/F4_emmeans_plot.jpg", plot = emmeans_plot, width = 12, height = 12, dpi = 300)


### From LM ###

# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)

######## DEALING W COLLINEARITY??

# Perform LM
lm_result <- lm(F4_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

cor_matrix <- cor(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], use = "complete.obs")
print(cor_matrix)

library(corrplot)
corrplot(cor_matrix, method = "circle")

library(car)
vif_values <- vif(lm_result)
print(vif_values)

pca <- prcomp(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], scale. = TRUE)
summary(pca)

# OR on correlated vars? i think better
correlated_vars <- roi_hep_data[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
correlated_vars_scaled <- scale(correlated_vars)
pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")  # Scree plot
roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

# updated model
lm_result_pca <- lm(F4_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary_lm_pca <- summary(lm_result_pca)
lm_coeff_pca <- summary(lm_result_pca)$coefficients
print(lm_result_pca)

ancova_result_pca <- aov(F4_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary(ancova_result_pca)
ancova_table <- as.data.frame(anova(ancova_result_pca))
emm <- emmeans(ancova_result_pca, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)
# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Summary of the ANCOVA result
summary(ancova_result)



# re asses multicollinearity
vif_values <- vif(lm_result)
print(vif_values)

#  save  results
#write.csv(lm_coeff_pca, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/lm_coefficients_F4.csv")
#write.csv(ancova_table, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/ancova_table_F4.csv")

###############

# Plot using ggplot2 with larger axis labels and a title
emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
  geom_line(size = 1.5) +          # Line for each group
  geom_point(size = 4) + # Points for each group
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
  scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
  labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
  ggtitle("F4 HEP Mean Amplitude") +  # Add the title
  theme_minimal() +      # Minimal theme for clean look
  theme(
    axis.title = element_text(size = 20),    # Increase axis title size
    axis.text = element_text(size = 20),     # Increase axis text size
    legend.text = element_text(size = 14),   # Increase legend text size
    legend.title = element_text(size = 16),  # Increase legend title size
    strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
    plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/plots_averaged_all_subs_typical_prep_baseline--0.12--0.03/F4_emmeans_plot.jpg", plot = emmeans_plot, width = 12, height = 12, dpi = 300)



```

Experiment:
check F4 without covars
```{r}


# Perform LM
lm_result <- lm(Fp2_mean_amplitude ~ group*task,  
                       data = roi_hep_data)

lm_result <- lm(Fp2_mean_amplitude ~ group,  
                       data = roi_hep_data_ec)



# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)



```

F8

```{r F8 ancova / lm / emmeans}

# Perform ANCOVA
ancova_result <- aov(F8_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

# Perform LM
lm_result <- lm(F8_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

# Summary of the ANCOVA result
summary(ancova_result)

emm <- emmeans(ancova_result, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)

# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Plot using ggplot2 with larger axis labels and a title
emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
  geom_line(size = 1.5) +          # Line for each group
  geom_point(size = 4) + # Points for each group
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
  scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
  scale_y_continuous(limits = c(-1, 1)) +  # Fixed y-axis scale (adjust limits as needed)
  labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
  ggtitle("F8 HEP Mean Amplitude") +  # Add the title
  theme_minimal() +      # Minimal theme for clean look
  theme(
    axis.title = element_text(size = 20),    # Increase axis title size
    axis.text = element_text(size = 20),     # Increase axis text size
    legend.text = element_text(size = 14),   # Increase legend text size
    legend.title = element_text(size = 16),  # Increase legend title size
    strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
    plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

print(emmeans_plot)

# SAVE
#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/plots_averaged_all_subs_typical_prep_baseline--0.12--0.03/F8_emmeans_plot.jpg", plot = emmeans_plot, width = 12, height = 12, dpi = 300)


### From LM ###

# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)

######## DEALING W COLLINEARITY??

# Perform LM
lm_result <- lm(F8_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

cor_matrix <- cor(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], use = "complete.obs")
print(cor_matrix)

corrplot(cor_matrix, method = "circle")

vif_values <- vif(lm_result)
print(vif_values)

pca <- prcomp(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], scale. = TRUE)
summary(pca)

# OR on correlated vars? i think better
correlated_vars <- roi_hep_data[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
correlated_vars_scaled <- scale(correlated_vars)
pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")  # Scree plot
roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

# updated model
lm_result_pca <- lm(F8_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary_lm_pca <- summary(lm_result_pca)
lm_coeff_pca <- summary(lm_result_pca)$coefficients
print(lm_result_pca)

ancova_result_pca <- aov(F8_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary(ancova_result_pca)
ancova_table <- as.data.frame(anova(ancova_result_pca))
emm <- emmeans(ancova_result_pca, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)
# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Summary of the ANCOVA result
summary(ancova_result)



# re asses multicollinearity
vif_values <- vif(lm_result_pca)
print(vif_values)

#  save  results
write.csv(lm_coeff_pca, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/lm_coefficients_F8.csv")
write.csv(ancova_table, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/ancova_table_F8.csv")

###############


```

Fp2

```{r Fp2 ancova / lm / emmeans}

# Perform ANCOVA
ancova_result <- aov(Fp2_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

# Perform LM
lm_result <- lm(Fp2_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

# Summary of the ANCOVA result
summary(ancova_result)

emm <- emmeans(ancova_result, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)

# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Plot using ggplot2 with larger axis labels and a title
emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
  geom_line(size = 1.5) +          # Line for each group
  geom_point(size = 4) + # Points for each group
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
  scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
  scale_y_continuous(limits = c(-1, 1)) +  # Fixed y-axis scale (adjust limits as needed)
  labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
  ggtitle("Fp2 HEP Mean Amplitude") +  # Add the title
  theme_minimal() +      # Minimal theme for clean look
  theme(
    axis.title = element_text(size = 20),    # Increase axis title size
    axis.text = element_text(size = 20),     # Increase axis text size
    legend.text = element_text(size = 14),   # Increase legend text size
    legend.title = element_text(size = 16),  # Increase legend title size
    strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
    plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

print(emmeans_plot)

# SAVE
# ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/Fp2_emmeans_plot.jpg", plot = emmeans_plot, width = 12, height = 12, dpi = 300)


### From LM ###

# Get the summary of the model to extract the coefficients
summary_lm <- summary(lm_result)
print(summary_lm)
# Extract coefficients, standard errors, t-values, and p-values
coefficients <- summary_lm$coefficients
# View the structure of coefficients to understand its layout
str(coefficients)
# Print the coefficients and their significance
print(coefficients)


######## DEALING W COLLINEARITY??

# Perform LM
lm_result <- lm(Fp2_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = roi_hep_data)

cor_matrix <- cor(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], use = "complete.obs")
print(cor_matrix)

library(corrplot)
corrplot(cor_matrix, method = "circle")

library(car)
vif_values <- vif(lm_result)
print(vif_values)

pca <- prcomp(roi_hep_data[, c("age", "education_years", "body_mass_index", "heart_rate_bpm", "hrv_rmssd_ms", "QT_interval_ms", "QTc_interval_ms", "R_peak_amplitude_mV", "MAIA_not_worrying")], scale. = TRUE)
summary(pca)

# OR on correlated vars? i think better
correlated_vars <- roi_hep_data[, c("heart_rate_bpm", "QT_interval_ms", "QTc_interval_ms")]
correlated_vars_scaled <- scale(correlated_vars)
pca_result <- prcomp(correlated_vars_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
plot(pca_result, type = "l")  # Scree plot
roi_hep_data$PC1 <- pca_result$x[, 1]  # First principal component
roi_hep_data$PC2 <- pca_result$x[, 2]  # Second principal component (if needed)

# updated model
lm_result_pca <- lm(Fp2_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary_lm_pca <- summary(lm_result_pca)
lm_coeff_pca <- summary(lm_result_pca)$coefficients
print(lm_result_pca)

ancova_result_pca <- aov(Fp2_mean_amplitude ~ group*task + age + sex + education_years + body_mass_index + smoker + had_caffeine + PC1 + PC2 + hrv_rmssd_ms + R_peak_amplitude_mV + MAIA_not_worrying, data = roi_hep_data)
summary(ancova_result_pca)
ancova_table <- as.data.frame(anova(ancova_result_pca))
emm <- emmeans(ancova_result_pca, ~ group * task)
pairwise_results <- pairs(emm)
summary(emm)
plot(emm)
# Convert emmeans results to a data frame
emm_df <- as.data.frame(emm)

# Summary of the ANCOVA result
summary(ancova_result)


# re asses multicollinearity
vif_values <- vif(lm_result)
print(vif_values)

#  save  results
#write.csv(lm_coeff_pca, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/lm_coefficients_Fp2.csv")
write.csv(lm_coeff_pca, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/lm_coefficients_Fp2.csv")
write.csv(ancova_table, "/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/ancova_table_Fp2.csv")

###############

# Plot using ggplot2 with larger axis labels and a title
emmeans_plot <- ggplot(emm_df, aes(x = task, y = emmean, color = group, group = group)) +
  geom_line(size = 1.5) +          # Line for each group
  geom_point(size = 4) + # Points for each group
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), width = 0.4, size = 1.2) + # Error bars
  scale_color_manual(values = c("#004d00","#d84a37")) +  # Custom colors
  labs(x = "Task", y = "Estimated Marginal Means", color = "Group") +      # Axis and legend labels
  ggtitle("Fp2 HEP Mean Amplitude") +  # Add the title
  theme_minimal() +      # Minimal theme for clean look
  theme(
    axis.title = element_text(size = 20),    # Increase axis title size
    axis.text = element_text(size = 20),     # Increase axis text size
    legend.text = element_text(size = 14),   # Increase legend text size
    legend.title = element_text(size = 16),  # Increase legend title size
    strip.text = element_text(size = 16),    # Increase size of facet labels (if any)
    plot.title = element_text(size = 20, hjust = 0.5)  # Title size and center alignment
  )

# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/HEP/MBB_results/plots_averaged_all_subs_typical_prep_baseline--0.12--0.03/Fp2_emmeans_plot.jpg", plot = emmeans_plot, width = 12, height = 12, dpi = 300)



```

ANCOVA for my 3 ROIs separately

```{r}

rois <- c("Fp2_hep_mean_amplitude", "F4_hep_mean_amplitude", "F8_hep_mean_amplitude")

# Initialize a list to store the results
combined_ancova_results <- list()

for (var in rois) {
  
  ancova_result <- aov(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = hep_ecg_beh_data_anocova,
                  na.action = na.omit)
  

  # Get the summary of the ANCOVA model to extract the coefficients
  ancova_summary <- summary(ancova_result)
  ancova_df <- as.data.frame(ancova_summary[[1]])
  # Add predictor and outcome columns
  ancova_df$predictor <- rownames(ancova_df)
  ancova_df$predictor <- trimws(ancova_df$predictor)
  rownames(ancova_df) <- NULL
  ancova_df$outcome <- rep(var, times = nrow(ancova_df))
  
  # Calculate df_den
  n <- nobs(ancova_result)
  k <- length(levels(data$group))
  df_den <- n - k
  ancova_df$df_den <- df_den
  
  # Rename columns
  colnames(ancova_df) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  ancova_df <- ancova_df[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

  # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(ancova_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  combined_table <- full_join(ancova_df, effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_ancova_results[[var]] <- combined_table
  
}

# Combine all results into one dataframe
combined_ancova_results_df_rois <- do.call(rbind, combined_ancova_results)

# Correct for multiple comparisons using the Benjamini-Hochberg procedure
#anova_results_df_questionnaires$p_adjusted <- p.adjust(anova_results_df_questionnaires$p, method = "BH") # add n_length!! https://stackoverflow.com/questions/30108510/p-adjust-with-n-than-number-of-tests

# Format p-values to avoid scientific notation
#anova_results_df_questionnaires$p_adjusted <- format(anova_results_df_questionnaires$p_adjusted, scientific = FALSE, digits = 3)

# View the results
#print(anova_results_df_questionnaires)

# SAVE !!!
write.csv(combined_ancova_results_df_rois, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/ancova_results_hep_3_rois.csv", row.names = FALSE)
```

2)  All channels: Exploratory

create exploded long data for all channels

```{r create a col for each }

df_copy <- hep_ecg_beh_data

# turn the channels and hep_mean_amplitudes to R-type lists...
df_copy$channels <- strsplit(as.character(df_copy$channels), ",")  

# Function to clean and extract numeric values
extract_numbers <- function(x) {
  # Remove newline characters and multiple spaces
  cleaned_string <- gsub("\\s+", " ", gsub("\\n", " ", x))
  
  # Remove leading "[" and trailing "]"
  cleaned_string <- gsub("^\\[|\\]$", "", cleaned_string)
  
  # Split by spaces
  numbers <- unlist(strsplit(cleaned_string, " "))
  
  # Remove empty strings and whitespace
  numbers <- numbers[nzchar(numbers)]
  
  # Filter valid numeric entries
  numbers <- numbers[grepl("^[+-]?\\d*\\.?\\d+([eE][+-]?\\d+)?$", numbers)]
  
  return(numbers)
}

# Apply the function to the column
df_copy$hep_mean_amplitudes <- lapply(df_copy$hep_mean_amplitudes, extract_numbers)


# Remove newline characters and extra whitespace
# df_copy$hep_mean_amplitudes <- gsub("\n", "", df_copy$hep_mean_amplitudes)
#df_copy$hep_mean_amplitudes <- trimws(df_copy$hep_mean_amplitudes)

# Explode lists into separate rows
hep_ecg_beh_data_exploded <- df_copy %>%
  mutate(row_id = row_number()) %>%
  unnest(cols = c(channels, hep_mean_amplitudes))

# Pivot the DataFrame
df_pivot <- hep_ecg_beh_data_exploded %>%
  select(-row_id) %>%
  pivot_wider(names_from = channels, values_from = hep_mean_amplitudes)


```

Clean colnames and make vars appropriate for stats

```{r clean data after explosion}

### clean the colnames

# Identify columns with single quotes
cols_to_clean <- grep("'", colnames(df_pivot), value = TRUE)
# indices
indices_to_clean <- which(grepl("'", colnames(df_pivot)))

# DOES NOT WORK WHEN DONE TOGETHER
df_pivot <- df_pivot %>%
  rename_with(~ ifelse(.x %in% cols_to_clean,
                       paste0(gsub("'", "", gsub("[\\[\\]]", "", .x)), "_hep_mean_amplitude"),
                       .x))  # Clean and add suffix only to specified columns

###### WORKS 

# Ensure no leading/trailing spaces in column names
colnames(df_pivot) <- trimws(colnames(df_pivot))

# Remove square brackets, single quotes, and any extra characters
colnames(df_pivot) <- gsub("\\[|\\]|'", "", colnames(df_pivot))

######

# everything worked give it a nice name
hep_ecg_beh_data_anocova <- df_pivot

# make numeric & convert microvolts to Volts
hep_ecg_beh_data_anocova[indices_to_clean] <- lapply(hep_ecg_beh_data_anocova[indices_to_clean], function(x) as.numeric(x) * 1e6)
# df_pivot[indices_to_clean] <- lapply(df_pivot[indices_to_clean], function(x) as.numeric(x) * 1e6)
# hep_ecg_beh_data_anocova[indices_to_clean] <- hep_ecg_beh_data_anocova[indices_to_clean] * 1e6

# Suppress scientific notation for display
options(scipen = 999)


```

Create the mean frontal-central HEP mean

```{r frontal_central_HEP_mean}
# Convert indices_to_clean to actual column names
columns_to_check <- colnames(hep_ecg_beh_data_anocova)[indices_to_clean]

# Filter columns that include any of the frontal_central_regions strings
cols_to_average <- columns_to_check[sapply(columns_to_check, function(col_name) {
  any(sapply(frontal_central_regions, function(region) grepl(region, col_name)))
})]

# create the average 
hep_ecg_beh_data_anocova$frontal_central_HEP_mean <- rowMeans(hep_ecg_beh_data_anocova[, cols_to_average], na.rm = TRUE)

```

ANCOVA MEAN All Frontal Central

```{r}
# Perform ANCOVA
ancova_result <- aov(frontal_central_HEP_mean ~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying, 
                       data = hep_ecg_beh_data_anocova)



# Get the summary of the ANCOVA model to extract the coefficients
ancova_summary <- summary(ancova_result)
ancova_df <- as.data.frame(ancova_summary[[1]])
# Add predictor and outcome columns
ancova_df$predictor <- rownames(ancova_df)
ancova_df$predictor <- trimws(ancova_df$predictor)
rownames(ancova_df) <- NULL
ancova_df$outcome <- rep("frontal_central_HEP_mean", times = nrow(ancova_df))

# Calculate df_den
n <- nobs(ancova_result)
k <- length(levels(hep_ecg_beh_data_anocova$group))
df_den <- n - k
ancova_df$df_den <- df_den

# Rename columns
colnames(ancova_df) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")

# Reorder the columns
ancova_df <- ancova_df[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

# Calculate the effect size (eta-squared)
effect_size <- etaSquared(ancova_result)
# print(effect_size)
# Extract df
effect_size_df <- as.data.frame(effect_size)

# Add predictor and outcome columns to the coefficients data frame
effect_size_df$predictor <- rownames(effect_size_df)
rownames(effect_size_df) <- NULL
effect_size_df$outcome <- rep("frontal_central_HEP_mean", times = nrow(effect_size_df))

# Rename coefficient columns for clarity
colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")


# Merge the ANOVA and coefficient tables on the predictor variable
combined_table <- full_join(ancova_df, effect_size_df, by = c("outcome", "predictor"))

## save
write.csv(combined_table, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/ancova_results_heps_averaged_frontal_central.csv", row.names = FALSE)


  
##### OLD #####

# Summary of the ANCOVA result
ancova_summary <- summary(ancova_result)
print(ancova_summary)
# Extract df
ancova_df <- as.data.frame(ancova_summary[[1]])

# Calculate the effect size (eta-squared)
effect_size <- etaSquared(ancova_result)
print(effect_size)
# Extract df
effect_size_df <- as.data.frame(effect_size)

# Combine the ANCOVA summary and effect sizes
result_df <- cbind(ancova_df, effect_size_df)

# Print the combined summary
# print(ancova_summary_with_effect_size)

## IF U WANT COEFSS u need to fit a lm model!

# Extract coefficients, standard errors, t-values, and p-values
ancova_coefficients <- ancova_summary$coefficients
# View the structure of coefficients to understand its layout
str(ancova_coefficients)
# Print the coefficients and their significance
print(ancova_coefficients)

```

ANCOVA ALL Frontal Central Chans Loop: NOTE: You could also consider adding the mean of all frontal central channels here as another "channel"?

```{r}
# Initialize a list to store the results
combined_ancova_results <- list()

for (var in cols_to_average) {
  
  ancova_result <- aov(as.formula(paste(var, "~ group*task + age + sex + education_years + body_mass_index  + smoker + had_caffeine  + 
                       heart_rate_bpm + hrv_rmssd_ms + QT_interval_ms  +  QTc_interval_ms  + R_peak_amplitude_mV + 
                       MAIA_not_worrying")), 
                  data = hep_ecg_beh_data_anocova,
                  na.action = na.omit)
  

  # Get the summary of the ANCOVA model to extract the coefficients
  ancova_summary <- summary(ancova_result)
  ancova_df <- as.data.frame(ancova_summary[[1]])
  # Add predictor and outcome columns
  ancova_df$predictor <- rownames(ancova_df)
  ancova_df$predictor <- trimws(ancova_df$predictor)
  rownames(ancova_df) <- NULL
  ancova_df$outcome <- rep(var, times = nrow(ancova_df))
  
  # Calculate df_den
  n <- nobs(ancova_result)
  k <- length(levels(data$group))
  df_den <- n - k
  ancova_df$df_den <- df_den
  
  # Rename columns
  colnames(ancova_df) <- c("df_num","sum_sq","mean_sq","f","p","predictor","outcome","df_den")
  
  # Reorder the columns
  ancova_df <- ancova_df[,c("outcome","predictor","sum_sq","mean_sq","df_num","df_den","f","p")]

  # Calculate the effect size (eta-squared)
  effect_size <- etaSquared(ancova_result)
  # print(effect_size)
  # Extract df
  effect_size_df <- as.data.frame(effect_size)
  
  # Add predictor and outcome columns to the coefficients data frame
  effect_size_df$predictor <- rownames(effect_size_df)
  rownames(effect_size_df) <- NULL
  effect_size_df$outcome <- rep(var, times = nrow(effect_size_df))
  
  # Rename coefficient columns for clarity
  colnames(effect_size_df) <- c("eta_sq", "partial_eta_sq", "predictor", "outcome")
  
  
  # Merge the ANOVA and coefficient tables on the predictor variable
  combined_table <- full_join(ancova_df, effect_size_df, by = c("outcome", "predictor"))
  
  # Store the combined results in the list
  combined_ancova_results[[var]] <- combined_table
  
}

# Combine all results into one dataframe
combined_ancova_results_df <- do.call(rbind, combined_ancova_results)

# Correct for multiple comparisons using the Benjamini-Hochberg procedure
#anova_results_df_questionnaires$p_adjusted <- p.adjust(anova_results_df_questionnaires$p, method = "BH") # add n_length!! https://stackoverflow.com/questions/30108510/p-adjust-with-n-than-number-of-tests

# Format p-values to avoid scientific notation
#anova_results_df_questionnaires$p_adjusted <- format(anova_results_df_questionnaires$p_adjusted, scientific = FALSE, digits = 3)

# View the results
#print(anova_results_df_questionnaires)

# SAVE !!!
#write.csv(combined_ancova_results_df, file = "/Users/denizyilmaz/Desktop/BrainTrain/Results/ancova_results_hep_all_frontal_central.csv", row.names = FALSE)



```

Cluster-Based Correction Analysis: Better on MNE???

```{r}

```

### 1.4) Control Analyses

As control analyses, we will compare the demographics and the ECG data of HC and SSD samples with independent samples Student's t-tests for continuous variables (Mann-Whitney U Test in case of assumption violation) and chi-squared test for categorical variables (Fisher's Exact Test if assumptions violated).

ECG

```{r}
# Convert the variable to numeric
roi_hep_data$ecg_mean_amplitude_time_window <- as.numeric(roi_hep_data$ecg_mean_amplitude_time_window)

# convert to mV
roi_hep_data$ecg_mean_amplitude_time_window <- roi_hep_data$ecg_mean_amplitude_time_window * 1e6

# Conduct a one-way ANOVA for ecg_mean_amplitude_time_window across different groups
anova_result <- aov(ecg_mean_amplitude_time_window ~ group, data = roi_hep_data)

# Summary of the ANOVA
summary(anova_result)
capture.output(summary(anova_result), file = paste0(plot_path, "anova_results_ecg.txt"))


# Calculate means for each group
group_means <- tapply(roi_hep_data$ecg_mean_amplitude_time_window, roi_hep_data$group, mean)
write.csv(group_means, paste0(plot_path, "group_means_ecg.csv"))



# Plot the results using boxplots to visualize group differences
ggplot(roi_hep_data, aes(x = group, y = ecg_mean_amplitude_time_window, fill = group)) +
  geom_boxplot() +
  labs(x = "Group", y = "ECG Mean Amplitude (Time Window)") +
  theme_minimal()

ggsave(paste0(plot_path, "ecg_mean_amplitude_by_group_boxplot.png"), width = 8, height = 6, dpi = 300)



```

## 2) Interoception measures will relate to global clinical outcomes (symptom severity measured by PANSS Total and cognitive deficits measured by BACS composite global scores) in patients with SSD.

The link between interoception measures (Interoceptive Sensibility, Interoceptive Accuracy, HEP amplitude) and global clinical outcomes (symptom severity measured by PANSS Total and cognitive deficits measured by BACS composite global scores) in patients with SSD, will be assessed by partial correlation analyses, controlling for age, gender, BMI, years of education, chlorpromazine equivalent dose, illness duration. For HCT and HEP analyses, we will also add smoking, caffeine intake, and clozapine dose as covariates.

CHECK ASSUMPTIONS TOO !!!

partial cor fun

```{r}
perform_partial_correlation <- function(data, columns_to_keep, x_col, y_col, control_vars) {
  
  # Subset the data to include only the specified columns
  partial_corr_data <- data %>%
    select(all_of(columns_to_keep)) %>%
    na.omit()  # Remove rows with missing values
  
  # Convert necessary columns to numeric for partial correlation
  partial_corr_data <- partial_corr_data %>%
    mutate(across(
      all_of(c("age", "sex", "education_years", "body_mass_index", x_col, y_col)),
      as.numeric
    )) %>%
    mutate(sex = as.numeric(as.factor(sex)))  # Convert to numeric if `sex.x` is categorical
  
  # Perform the partial correlation
  partial_corr_result <- pcor.test(
    x = partial_corr_data[[x_col]],
    y = partial_corr_data[[y_col]],
    z = partial_corr_data %>% select(all_of(control_vars))
  )
  
  return(partial_corr_result)
}

```

partial cor fun with task option

```{r}
perform_partial_correlation <- function(data, columns_to_keep, x_col, y_col, control_vars, task_filter_col = NULL, task_filter_value = NULL) {
  library(dplyr)
  library(ppcor)  # For partial correlation
  
  # Subset the data to include only the specified columns and apply the task filter if provided
  partial_corr_data <- data %>%
    filter(if (!is.null(task_filter_col) && !is.null(task_filter_value)) 
             !!sym(task_filter_col) == task_filter_value else TRUE) %>%
    select(all_of(columns_to_keep)) %>%
    na.omit()  # Remove rows with missing values
  
  # Convert necessary columns to numeric for partial correlation
  partial_corr_data <- partial_corr_data %>%
    mutate(across(
      all_of(c("age.x", "sex.x", "education_years.x", "body_mass_index.x", x_col, y_col)),
      as.numeric
    )) %>%
    mutate(sex = as.numeric(as.factor(sex.x)))  # Convert to numeric if `sex.x` is categorical
  
  # Perform the partial correlation
  partial_corr_result <- pcor.test(
    x = partial_corr_data[[x_col]],
    y = partial_corr_data[[y_col]],
    z = partial_corr_data %>% select(all_of(control_vars))
  )
  
  return(partial_corr_result)
}

```

###  Self reported Interoception & PANSS Total

TEST: interoceptive sensibility and PANSS total

```{r bayesian}
columns_partial_corr <-  c("PANSS_total", "MAIA_total","age", "sex", "education_years", "body_mass_index")

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- corr_questionnaire_clinical_data[, columns_partial_corr] # post_sports_data_no_na

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

p <- summary(partial_corr_results, redundant = TRUE)

# Specify the file path where you want to save the CSV file
file_path <- "/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/partial_corr_results_panss_maia.csv"

# Save the data frame to CSV
write.csv(p, file = file_path, row.names = FALSE)


```

```{r using the function}
columns_partial_corr <-  c("PANSS_total", "MAIA_total","age", "sex", "education_years", "body_mass_index")
control_vars <-  c("age", "sex", "education_years", "body_mass_index")

partial_corr_result_maia_panss <- perform_partial_correlation(corr_questionnaire_clinical_data, columns_partial_corr, "PANSS_total", "MAIA_total",  control_vars)
```

```{r hand coded }
# Specify columns to keep
columns_partial_corr <- c("PANSS_total", "MAIA_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- corr_questionnaire_clinical_data %>%
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    MAIA_total = as.numeric(MAIA_total)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$MAIA_total,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# Create subset of data_bl_complete_cases with specified columns
questionnaire_clinical_data <- columns_partial_corr[, columns_partial_corr]

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(partial = TRUE, bayesian = TRUE, method = "spearman",
              bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), 
              include_factors = TRUE, partial_bayesian = TRUE) 

# get summary
summary(partial_corr_results, redundant = TRUE)

# Extract rho for the partial correlation between "aerobic.fitness.2mmol" and "brain_age_gap"
rho_value <- partial_corr_results$rho[which(partial_corr_results$Parameter1 == "brain_age_gap" &  partial_corr_results$Parameter2 == "aerobic.fitness.2mmol")]

# Print the rho value
print(rho_value)

# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)

```

PLOT: interoceptive sensibility and PANSS total

```{r}

# Adjust for covariates
fit_y <- lm(MAIA_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
maia_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted PANSS vs MAIA Total",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted MAIA Total (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")


# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/maia_panss_plot.jpg", plot = maia_panss_plot, width = 12, height = 12, dpi = 300)

```

### Interoceptive Accuracy & PANSS Total

add heart_rate_bpm as a covar
```{r}
corr_questionnaire_clinical_data

# Subset the data to include only the necessary columns
hr_data <- roi_hep_clinical_data %>%
  filter(task == "hct")  # Filter for the specific task

# Merge heart_rate_bpm from hr_data with corr_questionnaire_clinical_data
corr_questionnaire_clinical_data_hr <- corr_questionnaire_clinical_data %>%
  left_join(
    hr_data %>% select(subject, heart_rate_bpm),  # Select only the key and desired column
    by = "subject"  # Specify the key for merging
  )



```

TEST: interoceptive accuracy and PANSS total
```{r bayesian}
columns_partial_corr <-  c("PANSS_total", "interoceptive_accuracy","age", "sex", "education_years", "body_mass_index", "heart_rate_bpm")

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- corr_questionnaire_clinical_data_hr[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


```

```{r using the function}
columns_partial_corr <-  c("PANSS_total", "MAIA_total","age", "sex", "education_years", "body_mass_index", "heart_rate_bpm")
control_vars <-  c("age", "sex", "education_years", "body_mass_index", "heart_rate_bpm")

partial_corr_result_iacc_panss <- perform_partial_correlation(corr_questionnaire_clinical_data_hr, columns_partial_corr, "PANSS_total", "MAIA_total",  control_vars)
```

```{r}

# Specify columns to keep
columns_partial_corr <- c("PANSS_total", "interoceptive_accuracy","age", "sex", "education_years", "body_mass_index", "heart_rate_bpm")

# Subset the data to include only the necessary columns
partial_corr_data <- iacc_clinical_data %>%
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    interoceptive_accuracy = as.numeric(interoceptive_accuracy)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$interoceptive_accuracy,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: interoceptive acuracy and PANSS total

```{r}

data_partial_corr_clean <- data_partial_corr %>%
  select(interoceptive_accuracy, PANSS_total, age, sex, education_years, body_mass_index) %>%
  na.omit()

# Adjust for covariates
fit_y <- lm(interoceptive_accuracy ~ age + sex + education_years + body_mass_index, data = data_partial_corr_clean, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = data_partial_corr_clean, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
iacc_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS vs. Interoceptive Accuracy",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted IAcc (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/iacc_panss_plot.jpg", plot = iacc_panss_plot, width = 12, height = 12, dpi = 300)

```

###  HEPs & PANSS Total

TEST: PANSS Total and Fp2 EC

```{r}

# Specify columns to keep
columns_partial_corr <- c("Fp2_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(Fp2_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "Fp2_mean_amplitude","age", "sex", "education_years", "body_mass_index") # "heart_rate_bpm"

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and Fp2 EC

```{r}

# Adjust for covariates
fit_y <- lm(Fp2_mean_amplitude ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
F4_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS Total vs. F4 Mean Amplitude Eyes-Closed",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted Fp2 Mean Amplitude (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/Fp2_ec_panss_plot.jpg", plot = F4_ec_panss_plot, width = 12, height = 12, dpi = 300)

```

TEST: PANSS Total and F4 EC

```{r}

# Specify columns to keep
columns_partial_corr <- c("F4_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(F4_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "F4_mean_amplitude","age", "sex", "education_years", "body_mass_index") #  "heart_rate_bpm"

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F4 EC

```{r}

# Adjust for covariates
fit_y <- lm(F4_mean_amplitude ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
F4_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS Total vs. F4 Mean Amplitude Eyes-Closed",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted F4 Mean Amplitude (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/F4_ec_panss_plot.jpg", plot = F4_ec_panss_plot, width = 12, height = 12, dpi = 300)

```

TEST: PANSS Total and F8 EC

```{r}

# Specify columns to keep
columns_partial_corr <- c("F8_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-closed") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(F8_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)

# bayesian
columns_partial_corr <-  c("PANSS_total", "F8_mean_amplitude","age", "sex", "education_years", "body_mass_index") # , "heart_rate_bpm" 

# Create subset of data_bl_imputed with specified columns
data_partial_corr <- roi_hep_clinical_data[, columns_partial_corr] 

# here instead of iris make a df withh all ur vars of interst..
partial_corr_results <- data_partial_corr %>%
  correlation(method="pearson", bayesian=TRUE,  bayesian_prior = "medium.narrow", bayesian_ci_method = "hdi", bayesian_test = c("pd", "rope", "bf"), include_factors = TRUE, partial=TRUE,  partial_bayesian = TRUE)

summary(partial_corr_results, redundant = TRUE)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F8 EC

```{r}

# Adjust for covariates
fit_y <- lm(F8_mean_amplitude ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
F4_ec_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS Total vs. F4 Mean Amplitude Eyes-Closed",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted F8 Mean Amplitude (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/TAC_2_analysis/corrs/F8_ec_panss_plot.jpg", plot = F4_ec_panss_plot, width = 12, height = 12, dpi = 300)

```

TEST: PANSS Total and F4 EO

```{r}

# Specify columns to keep
columns_partial_corr <- c("F4_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "eyes-open") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(F4_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F4 EO

```{r}

# Adjust for covariates
fit_y <- lm(F4_mean_amplitude ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
F4_eo_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS Total vs. F4 Mean Amplitude Eyes-Open",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted F4 Mean Amplitude Eyes Open (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/MBB_results/F4_eo_panss_plot.jpg", plot = F4_eo_panss_plot, width = 12, height = 12, dpi = 300)

```

TEST: PANSS Total and F4 HCT

```{r}

# Specify columns to keep
columns_partial_corr <- c("F4_mean_amplitude", "PANSS_total","age", "sex", "education_years", "body_mass_index")

# Subset the data to include only the necessary columns
partial_corr_data <- roi_hep_clinical_data %>%
  filter(task == "hct") %>%  # Filter for the specific task
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age),
    sex = as.numeric(as.factor(sex)),  # Convert to factor first if needed
    education_years = as.numeric(education_years),
    body_mass_index = as.numeric(body_mass_index),
    PANSS_total = as.numeric(PANSS_total),
    F4_mean_amplitude = as.numeric(F4_mean_amplitude)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$PANSS_total,
  y = partial_corr_data$F4_mean_amplitude,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: PANSS Total and F4 HCT

```{r}

# Adjust for covariates
fit_y <- lm(F4_mean_amplitude ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(PANSS_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
F4_hct_panss_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  PANSS Total vs. F4 Mean Amplitude Heartbeat Counting Task",
       x = "Adjusted PANSS Total (Residuals)",
       y = "Adjusted F4 Mean Amplitude Heartbeat Counting Task (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
#ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/MBB_results/F4_hct_panss_plot.jpg", plot = F4_hct_panss_plot, width = 12, height = 12, dpi = 300)

```


### Interoception different levels corrs. with one another. 


TEST: interoceptive accuracy and MAIA total

```{r}

# Specify columns to keep
columns_partial_corr <- c("MAIA_total.x", "interoceptive_accuracy.x","age.x", "sex.x", "education_years.x", "body_mass_index.x")

# Subset the data to include only the necessary columns
partial_corr_data <- iacc_questionnaire_data %>%
  select(all_of(columns_partial_corr)) %>%
  na.omit()  # Remove rows with missing values, as partial correlation requires complete data

partial_corr_data <- partial_corr_data %>%
  mutate(
    age = as.numeric(age.x),
    sex = as.numeric(as.factor(sex.x)),  # Convert to factor first if needed
    education_years = as.numeric(education_years.x),
    body_mass_index = as.numeric(body_mass_index.x),
    MAIA_total = as.numeric(MAIA_total.x),
    interoceptive_accuracy = as.numeric(interoceptive_accuracy.x)
  )

# Perform the partial correlation
partial_corr_result <- pcor.test(
  x = partial_corr_data$MAIA_total,
  y = partial_corr_data$interoceptive_accuracy,
  z = partial_corr_data[, c("age", "sex", "education_years", "body_mass_index")]
)

# Print the result
print(partial_corr_result)


# save
# Specify the file path where you want to save the CSV file
# file_path <- "/Users/denizyilmaz/Desktop/BrainAGE/Results/partial_corr_results_fitness_brainage.csv"

# Save the data frame to CSV
# write.csv(partial_corr_results, file = file_path, row.names = FALSE)
```

PLOT: interoceptive acuracy and MAIA total

```{r}

# Adjust for covariates
fit_y <- lm(interoceptive_accuracy ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)
fit_x <- lm(MAIA_total ~ age + sex + education_years + body_mass_index, data = partial_corr_data, na.action=na.omit)

# Get residuals
residuals_y <- residuals(fit_y)
residuals_x <- residuals(fit_x)

# Create a data frame of residuals
residuals_df <- data.frame(residuals_x, residuals_y)

# Compute correlation coefficient
correlation_coefficient_residuals_maia_panss <- cor(residuals_x, residuals_y)

# Plot residuals
iacc_maia_plot <- ggplot(residuals_df, aes(x = residuals_x, y = residuals_y)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Partial Regression: Adjusted  MAIA Total vs. Interoceptive Accuracy",
       x = "Adjusted MAIA Total (Residuals)",
       y = "Adjusted IAcc (Residuals)") +
  theme_minimal() + 
    theme(
    plot.title = element_text(size = 20),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.text = element_text(size = 20)
  ) +
  annotate("text", x = Inf, y = Inf, label = paste0("r = ", round(correlation_coefficient_residuals_maia_panss, 2)), 
         hjust = 1.1, vjust = 2, size = 7, color = "blue")



# SAVE
ggsave("/Users/denizyilmaz/Desktop/BrainTrain/Results/correlations/MBB_results/iacc_maia_plot.jpg", plot = iacc_maia_plot, width = 12, height = 12, dpi = 300)

```


# ECG Control Analysis !!!

Compare the Evoked (Mean) ECGs that I masked for the HEP time points, to show no differences in the HEP window For this use the ecg_outputs csv s and you can find the mean amplitudes of ECG there.
